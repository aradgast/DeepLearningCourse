{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aradgast/DeepLearningCourse/blob/main/Assignment1_aradVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "load_from_drive = False\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/DataBase/YearPredictionMSD.txt'#.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "aebca2c7-7f5d-43a9-c989-7186e0510993"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
              "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
              "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
              "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
              "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...        ...   \n",
              "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
              "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
              "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
              "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
              "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
              "\n",
              "            var84     var85     var86      var87     var88      var89  \\\n",
              "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
              "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
              "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
              "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
              "...           ...       ...       ...        ...       ...        ...   \n",
              "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
              "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
              "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
              "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
              "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "515340 -15.46052  \n",
              "515341  10.88815  \n",
              "515342  -8.09364  \n",
              "515343  39.74909  \n",
              "515344  12.17352  \n",
              "\n",
              "[515345 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a69f060-9026-41b1-82e5-acb0f80b0b92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>...</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>...</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>...</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a69f060-9026-41b1-82e5-acb0f80b0b92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a69f060-9026-41b1-82e5-acb0f80b0b92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a69f060-9026-41b1-82e5-acb0f80b0b92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "72c8ef39-181f-43b6-d930-24b35f68f0d8"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here -> our explanation:\n",
        "# Parts of the var are reffering to the artist's data, for instance : the artist's id, artist's  location, and others...\n",
        "# those featchures might be studied by the neural network and can help it understand the correct outcome\n",
        "# in those cases, of course that the neural network will succeed in recognition of the test set\n",
        "# but it's only imply that the neural networks working good at data that is similar to the training set and\n",
        "# the result of the test will be faulty.\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "# when we calculate expected value by mean, we want to use as many samples as we can, the more samples by 'large numbers law' the better the estimation is.   \n",
        "# the same for the estimation of variance.\n",
        "# because the test set is with the same nature if the train set, we assume that the variance and expactancy are the same.\n",
        "# for this reason we will use the mean and std we computted with the train set, also for the test set.\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here ->\n",
        "# we should use the validation set to avoid overfitting - not letting the model\n",
        "# to 'memorized' our data set and by so preventing a flase low loss cause\n",
        "# by overfitting.\n",
        "# \n",
        "#\n",
        "#\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  e = 0.0001\n",
        "  return -t * np.log(y+e) - (1 - t) * np.log(1 - y + e)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  return sigmoid(np.dot(X,np.transpose(w)) + b)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred(np.zeros(90), 1, np.ones([2, 90]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD0655W16X02",
        "outputId": "217ae1b3-e935-42fd-dbd1-9b90f3f2419e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.73105858])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "        \n",
        "  \"\"\"\n",
        "  \n",
        "  err = y-t\n",
        "  dLdb = np.mean(err)\n",
        "  dLdw = np.dot(np.transpose(X),err)/len(err)\n",
        "  return dLdb , dLdw\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "The cost function is: \n",
        "\n",
        "$$L = \\frac{1}{N}âˆ‘_{(i=1)}^{N} -t_{i}log{y_{i}}-(1-t_{i})log{{(1-y_{i})}}$$\n",
        "\n",
        "by using partial dertitve we can get,\n",
        "\n",
        "$$ \\frac{âˆ‚L}{âˆ‚b} =  \\frac{âˆ‚L}{âˆ‚y}\\frac{âˆ‚y}{âˆ‚z}\\frac{âˆ‚z}{âˆ‚b}\\quad,y=Ïƒ(z), z = w^Tx+b     $$\n",
        "where each part can be evaluted elmentwise,\n",
        "\n",
        "$$ \\frac{âˆ‚L}{âˆ‚y_{i}} =  \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} $$\n",
        "$$ \\frac{âˆ‚y}{âˆ‚z} =  Ïƒ(z)(1-Ïƒ(z)) = y(1-y) $$\n",
        "$$ \\frac{âˆ‚z}{âˆ‚b} =  1 $$\n",
        "\n",
        "in total we get,\n",
        "$$ \\frac{âˆ‚L}{âˆ‚b} =  \\frac{1}{N}âˆ‘-t_i(1-y_i)+y_i(1-t_i) =\\frac{1}{N}âˆ‘y_i-t_i   $$\n",
        "if we sign, error = y - t, will get,\n",
        "$$ \\frac{âˆ‚L}{âˆ‚b} = mean(error) $$\n",
        "\n",
        "in the same way we could evalute the derivative in respect to w,\n",
        "\n",
        "$$ \\frac{âˆ‚L}{âˆ‚w} =  \\frac{âˆ‚L}{âˆ‚y}\\frac{âˆ‚y}{âˆ‚z}\\frac{âˆ‚z}{âˆ‚w}\\quad\n",
        ",y=Ïƒ(z), z = w^Tx+b     $$\n",
        "$$ \\frac{âˆ‚L}{âˆ‚y_{i}} =  \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} $$\n",
        "$$ \\frac{âˆ‚y}{âˆ‚z} =  Ïƒ(z_i)(1-Ïƒ(z_i)) $$\n",
        "$$ \\frac{âˆ‚z}{âˆ‚w_i} =  x_i $$\n",
        "\n",
        "in total we get,\n",
        "$$ \\frac{âˆ‚L}{âˆ‚w} = \\frac{1}{N}error^T * X $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "outputId": "7e7718de-6f40-48e0-c6e1-03ea962e6316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code goes here\n",
        "h = 0.000001\n",
        "w = np.zeros(90)\n",
        "b = 1\n",
        "x = np.ones([2, 90])\n",
        "t = np.ones([2,])\n",
        "y = pred(w,b,x)\n",
        "y_h = pred(w, b+h, x)\n",
        "derivative = derivative_cost(x, y, t)\n",
        "\n",
        "\n",
        "\n",
        "r1 = (cost(y_h,t)-cost(y,t))/h\n",
        "r2 = derivative[0]\n",
        "\n",
        "\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - -0.268904540134951\n",
            "The algorithm results is -  -0.2689414213699951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "outputId": "69cc3d90-845a-4c1c-fe7f-337d6675e892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "\n",
        "r1 = np.zeros([90,])\n",
        "for i in range(len(w)):\n",
        "  h_vec = np.zeros(w.shape)\n",
        "  h_vec[i] = h\n",
        "  y_h = pred(w+h_vec,b,x)\n",
        "  r1[i] = ((cost(y_h,t)-cost(y,t))/h)\n",
        "\n",
        "\n",
        "r2 = derivative[1]\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - [-0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454]\n",
            "The algorithm results is -  [-0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # declare as a global vars\n",
        "  global train_norm_xs\n",
        "  global train_ts\n",
        "  global val_norm_xs\n",
        "  global val_ts\n",
        "\n",
        "  train_x = train_norm_xs\n",
        "  train_label = train_ts\n",
        "  valid_x = val_norm_xs\n",
        "  valid_label = val_ts\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  vec_cost = []\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_x))\n",
        "    train_x = train_x[reindex]\n",
        "    train_label = train_label[reindex]\n",
        "\n",
        "    \n",
        "\n",
        "    for i in range(0, len(train_x), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_x[i:(i + batch_size)]\n",
        "      t = train_label[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      y = pred(w, b, X)\n",
        "\n",
        "      # update w and b\n",
        "      dLdb , dLdw = derivative_cost(X, y,t)\n",
        "      b -= mu*dLdb\n",
        "      w -= mu*dLdw\n",
        "    # increment the iteration count\n",
        "    iter += 1\n",
        "    # compute and print the *validation* loss and accuracy\n",
        "    if (iter % 10 == 0):\n",
        "      reindex_val = np.random.permutation(len(valid_x))\n",
        "      valid_x = valid_x[reindex_val]\n",
        "      valid_label = valid_label[reindex_val]\n",
        "      valid_x = valid_x[:15000]\n",
        "      valid_label = valid_label[:15000]\n",
        "      val_y = pred(w, b, valid_x)\n",
        "      val_cost = cost(val_y, valid_label)\n",
        "      vec_cost.append(val_cost)\n",
        "      val_acc = get_accuracy(val_y,valid_label)\n",
        "      # print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "              # iter, val_acc * 100, val_cost))\n",
        "\n",
        "    if iter >= max_iters:\n",
        "      break\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "      #need to be added for plotting\n",
        "      \n",
        "  return b,w, vec_cost, val_acc\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "outputId": "691d3746-e635-4836-d322-7f64513246aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w0 = np.zeros([90,])\n",
        "b0 = 0\n",
        "\n",
        "# Write your code here\n",
        "b,w,cost_result_001, val_acc_001 = run_gradient_descent(w0, b0, mu=0.01, batch_size=150)\n",
        "print(f'validation accuracy for mu = 0.01 -> {val_acc_001}')\n",
        "b,w,cost_result_01, val_acc_01 = run_gradient_descent(w0, b0) #mu = 0.1\n",
        "print(f'validation accuracy for mu = 0.1 -> {val_acc_01}')\n",
        "b,w,cost_result_02, val_acc_02 = run_gradient_descent(w0, b0, mu=0.2) \n",
        "print(f'validation accuracy for mu = 0.2 -> {val_acc_02}')\n",
        "b,w,cost_result_05, val_acc_05 = run_gradient_descent(w0, b0, mu=0.5) \n",
        "print(f'validation accuracy for mu = 0.5 -> {val_acc_05}')\n",
        "b,w,cost_result_1, val_acc_1 = run_gradient_descent(w0, b0, mu = 1)\n",
        "print(f'validation accuracy for mu = 1 -> {val_acc_1}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation accuracy for mu = 0.01 -> 0.7294666666666667\n",
            "validation accuracy for mu = 0.1 -> 0.7290666666666666\n",
            "validation accuracy for mu = 0.2 -> 0.7234666666666667\n",
            "validation accuracy for mu = 0.5 -> 0.7103333333333334\n",
            "validation accuracy for mu = 1 -> 0.6730666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "iter_vec = [(i+1)*10 for i in range(len(cost_result_001))]\n",
        "plt.plot(iter_vec, cost_result_001, label='mu = 0.01')\n",
        "plt.plot(iter_vec,cost_result_01, label='mu = 0.1')\n",
        "plt.plot(iter_vec,cost_result_02, label='mu = 0.2')\n",
        "plt.plot(iter_vec,cost_result_05, label='mu = 0.5')\n",
        "plt.plot(iter_vec, cost_result_1, label='mu = 1')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cost value')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mIAFIZEWoQp_",
        "outputId": "b5bd52e5-49ef-48bd-e69d-ca5ab11128fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVfrA8e+ZSe89gSSkQyCU0JsiIFKkC6isvf5WRcSyrqvu6uqKZe2Aruii4opIkyYWLKFKQgsdQkgCJJDee2bm/P5IQMAAA5nJlJzP8+RJZm575yaZd+4957xHSClRFEVRlAtpLB2AoiiKYp1UglAURVGapRKEoiiK0iyVIBRFUZRmqQShKIqiNMvB0gGYSkBAgIyMjLR0GC1SVVWFu7u7pcOwGup8nE+dj9+pc3G+lpyPnTt3FkopA5tbZjcJIjIykh07dlg6jBZJSkpi6NChlg7DaqjzcT51Pn6nzsX5WnI+hBDHL7ZM3WJSFEVRmqUShKIoitIslSAURVGUZtlNG0RzGhoayM7Opra21tKhGMXb25tDhw5ZOgyjubi4EBYWhqOjo6VDURTFDOw6QWRnZ+Pp6UlkZCRCCEuHc1kVFRV4enpaOgyjSCkpKioiOzubqKgoS4ejKIoZ2PUtptraWvz9/W0iOdgaIQT+/v42c3WmKMqVs+sEAajkYEbq3CqKfbP7BKEoimIOZQU1pO/Mt3QYZqUShJ2SUjJz5kxiY2Pp3r07u3btana9nTt30q1bN2JjY5k5cyZn5gdZunQpCQkJaDQamx+AqCjmsHVFOj98vJ/cjDJLh2I2KkHYqe+++46jR49y9OhR5s+fz0MPPdTseg899BAff/zx2XW///57ALp27cqKFSsYMmRIa4atKDahrrqBrH2FAGxZlo69TrymEoQZZWVlER8fz913303Hjh257bbb+Omnnxg8eDBxcXGkpKQA8OKLL/Lmm2+e3a5r165kZWW16NirVq3izjvvRAjBgAEDKC0t5fTp0+etc/r0acrLyxkwYABCCO68805WrlwJQOfOnenUqVOLYlAUe3VsdwEGnaTLNe3JzSgjY3eBpUMyC7vu5nquf645wMFT5SbdZ5f2XrwwPuGS66Snp7N06VIWLFhA3759WbRoEZs3b2b16tXMnj377BuyMW655RaOHDnyh+efeOIJ7rzzzvOey8nJITw8/OzjsLAwcnJyaNeu3XnrhIWF/WEdRVEuLS0lF+8gV66b3pHcjDK2fnOMyO4BaB3s6zN3m0kQlhIVFUW3bt0ASEhI4Prrr0cIQbdu3a74KuHrr782Q4SKolyJypI6ctJK6Ts2Co1Ww6CbYlk7dw/7N+TQ4/rwy+/AhpgtQQghFgDjgHwpZddmlscDnwK9gOeklG+es2w08B6gBT6RUr7W0ngu90nfXJydnc/+rNFozj7WaDTodDoAHBwcMBgMZ9e72NiCK7mCCA0N5eTJk2cfZ2dnExoa+od1srOzL7mOoijnO7o9DyR07BcMQIcEP8Lifdm+LpNOA0JwcbefygLmvB76DBh9ieXFwEzgzXOfFEJogXnAGKALMF0I0cVMMVqFyMjIs72Mdu3aRWZmZrPrff3116Smpv7h68LkADBhwgQWLlyIlJJt27bh7e193u0lgHbt2uHl5cW2bduQUrJw4UImTpxo+heoKHYkbXsuQZFe+AS5AY3jgQZNiaWuWsfO7y9aOdsmmS1BSCk30pgELrY8X0q5HWi4YFE/IF1KmSGlrAcWA3b9rjVlyhSKi4vp168fc+fOpWPHji3e54033kh0dDSxsbE88MADfPDBB2eXJSYmnv35gw8+4P777yc2NpaYmBjGjBkDwDfffENYWBi//fYbY8eOZdSoUS2OSVFsXfGpKgpPVp69ejgjMNyT+AEh7P31JOWFNRaKzvSEObtnCSEigbXN3WI6Z50Xgcozt5iEEFOB0VLK+5se3wH0l1LOaGbbB4EHAYKDg3svXrz4vOXe3t7Exsaa5LW0Br1ej1artXQYVyQ9PZ2yMvP0A6+srMTDw8Ms+7ZF6nz8zlLnIm+vgcJD0GmiwMHl/EoCDdWSo99KPEMhfFDrNla35HwMGzZsp5SyT3PLbLqRWko5H5gP0KdPH3nhjEqHDh2ymeJ3YFvF+s5wcXGhZ8+eZtm3mjXsfOp8/M4S50JKyRfrf6NDZzdGjE5sdh0vXQY71mXReXovgqO8Wi02c50Pa+yTlQOc2xUgrOk5RVEUi8k9VkZFUS1xF9xeOlfPkR1w9XRky/KjdjF4zhoTxHYgTggRJYRwAm4FVls4JkVR2ri0lDwcHDVEJwZedB0nFwf6jY/mdHoZmXsKWzE68zBnN9evgKFAgBAiG3gBcASQUv5HCBEC7AC8AIMQYhbQRUpZLoSYAfxAYzfXBVLKA+aKU1EU5XL0egPpO/OJ7BGAk8ul3za7DG7H3l9OsnVFOhHd/NFqrfFzuHHMliCklNMvszyXxttHzS1bB6wzR1yKoihX6uSBYmqrGujYL+Sy62q0GgZNieXbeXs5sPEU3Yc1+zZnE2w3tSmKorSStJRcnN0d6NDFz6j1I7r6E9rJl+1rM6mr0Zk5OvNRCcJOGVvu+7nnniM8PFx1n1SUi6iv1ZG5p5DY3sFG11oSQjB4Siy11Q3s+j7LvAGakUoQdsrYct/jx48/W1VWUZQ/ykwtQNdg+MPguMsJ7OBJp/4h7Pk5m/Ii2xw8pxKEGVl7uW+AAQMG/KEEh6Iov0tLycPTz4V20d5XvG3/CdEgIHlVhhkiMz+bHih3Rb57BnL3mXafId1gzKXrCFpzuW9FUS6turyek4dL6DmyA0Jz5XOwe/q5kHh9ODu/P06P68MJimi9wXOm0HYShIWoct+KYrvSd+YhDfKKby+dq9eoCA5uOcWWZelMeqInQlx5orGUtpMgLvNJ31ysudy3oiiXlpaSh3+YB/7tr74Th5OrA33HRrFxcRpZewuJ6nHxgXbWpu0kCCsWGRnJ2rVrgcuX+zbWhAkTmDt3LrfeeivJycnNlvtWFOXiSvOrycssZ+BNMS3eV5dr27P312y2rjhGh662M3jONqK0c5Ys9/30008TFhZGdXU1YWFhvPjiiy0+tqLYg6Pb80BAx75Xf3vpDK1Ww6CbYijNq+bgplMmiK51qCsIM4qMjGT//v1nH3/22WfNLnN1deXHH380aTVXIQTz5s1rdllqaurZn9944w3eeOMNkxxTUeyFlJK0lDxC43zw8HUxyT4juwfQPs6HlLWZdOwfgrOr9b/9qisIRVGUCxScqKA0r9qo0hrGEkIweGostZUN7PrBNmaeUwlCURTlAmnJeWgcBDG9TNugHBThRcd+wez5+SQVxc13RrEmKkEoiqKcw2CQHN2RR2TXAJzdHE2+//4To0HaxuA5lSAURVHOkXO4hOry+haNfbgUL39XelwfxpHkXApOVJjlGKaiEoSiKMo50lJycXJ1IKKbv9mO0Wt0JC7ujmxZZt0zz6kEoSiK0kRXr+dYagExPQNxcNSa7TjOrg70HRdFTlopx/cVme04LaUShJ0yptx3dXU1Y8eOJT4+noSEBJ555hkLRKoo1iNzbyENtXqz3V46V8KQ9ngHubJ1RToGveHyG1iAShB2ythy30899RSHDx9m9+7dbNmyhe+++66VI1UU65GWkoe7txPtO/qa/ViNg+diKcmt5uCWP1ZatgYqQZiRtZf7dnNzY9iwYQA4OTnRq1cvsrOzW3RcRbFVtVUNnDhQRFzfYDRXUbn1akT1CKBdrDcpazKot8KZ56x/KJ+JvJ7yOoeLD5t0n/F+8fy1318vuY6tlPsuLS1lzZo1PPbYY0bHoyj2JH1nPga9NOnguMtpnHkujmWv72DXj8cZMLHldZ9Mqc0kCEuxhXLfOp2O6dOnM3PmTKKjo81yDEWxdmkpufiGuBEQ3rrT7wZHeRHXN5jUn07SdUioyUp7mEKbSRCX+6RvLrZQ7vvBBx8kLi6OWbNmGfmqFMW+lBfVcDq9jP4ToiwyX8OAidEc251P8qoMrr+7S6sf/2LaTIKwZpYs9/38889TVlbGJ598cnXBK4odOLo9D4C4vq13e+lcXgGu9BgWzu6fTtB9eDiBHUxTtLOlVCO1FbBUue/s7GxeeeUVDh48SK9evUhMTFSJQmmT0lLyCIn2wjvQ1WIx9B4TgbObA1uWp1vN4Dl1BWFG1l7uOywszGr+EBXFUgqzKyk+VcWQW1v+wawlnN0c6Ts2is1LjnJ8fxGR3QIsGg+oKwhFUdq4tJRchEYQ2zvI0qHQdUgo3oGubF1xzCoGz6kEoShKmyUNkqPb8+jQxQ9XTydLh4PWQcPAm2IoOV3Foa2WHzynEoSiKG3WqfRSKkvqWqW0hrGiEwNpF+NN8ppM6mstO3hOJQhFUdqstJQ8HJy1RPUw7cRALSGEYNDUWGrK69n94wmLxqIShKIobZK+wcCxXflE9wjA0dl8lVuvRkiUN7F9gkhdf4LKkjqLxWG2BCGEWCCEyBdC7L/IciGEeF8IkS6E2CuE6HXOMr0QIrXpa7W5YlQUpe06fqCIumpdq5bWuBIDJ8VgkJLkNZabec6cVxCfAaMvsXwMENf09SDw4TnLaqSUiU1fE8wXov0yptw3wNChQ+nUqROJiYkkJiaSn5/fypEqimWkpeTh6ulIeGfzV269Gl4BrnQfGsbh305TmG2ZmefMliCklBuB4kusMhFYKBttA3yEEM1XklOumLHlvgG+/PJLUlNTSU1NJSjI8l39FMXc6mt0ZO0rJLZ3MBqt9d5p7z0mEmdXB7Yss8zgOUsOlAsFTp7zOLvpudOAixBiB6ADXpNSNlvyVAjxII1XHwQHB5OUlHTecm9vbyoqLDfn6/Hjx7npppvo27cvycnJ9OrVi9tvv53Zs2dTUFDAJ598Qp8+fZg9ezYeHh488sgjVFRU0L9/f5YsWUJERMRVH3vZsmVMmzaNyspKEhISKC4u5ujRo4SEnH85rdfrqaqquurzVFtb+4fzbiqVlZVm27ctUufjdy09FyUZEn2DpNIhh6SkU6YLzAx8O0myd5ewdnESnu2arxNlrr8Nax1JHSGlzBFCRAO/CCH2SSmPXbiSlHI+MB+gT58+cujQoectP3To0NmRybmzZ1N3yLTlvp07xxPy7LMXXe7h4UFGRgbLly8nISGBvn37snLlSn777TdWr17Ne++9x8qVK3F2dsbZ2RmtVounpycajQYPD48/jKq+kmJ9+fn5dOzY8ew+OnToQFlZGXFxceetp9VqmTFjBlqtlilTpvD8889fUbEyFxcXevbsafT6VyIpKYkLf6dtmTofv2vpuViVuhuvwFrGTBlgkeJ8V0J/jYFF/0ym8qiGsTf3bfaKx1x/G5ZMEDlA+DmPw5qeQ0p55nuGECIJ6An8IUHYAmsv9/3ll18SGhpKRUUFU6ZM4YsvvvhDslEUe1JVVkfOkRJ6j4m0+uQATYPnJsXww8f7OfxbLl2uad9qx7ZkglgNzBBCLAb6A2VSytNCCF+gWkpZJ4QIAAYDb7T0YJf6pG9O1l7u+8xznp6e/OlPfyIlJUUlCMWuHd2eh5RY1eC4y4npFUhItBfJqzOI7ROEk0vrvHWb7ShCiK+AoUCAECIbeAFwBJBS/gdYB9wIpAPVwD1Nm3YGPhJCGGhsRH9NSnnQXHFaA0uV+9bpdJSWlhIQEEBDQwNr165lxIgRV/9CFMUGpKXkEdjBE98Qd0uHYjQhBIOnxrH8jZ2krj9Bv/GtM7GX2RKElHL6ZZZL4JFmnt8KdDNXXNZoypQpLFy4kH79+jFw4ECTlftet24dsbGxuLm58emnn55dlpiYSGpqKnV1dYwaNYqGhgb0ej0jRozggQceaPGxFcValeRWUXCigsFTYy0dyhULifYmplcQu9efIOHaUNx9nC+/UQtZayO1XbD2ct/u7u7s3LnTJMdTFFuQlpKHEBDX13ZuL51r4ORoMvcUkLwmg+F3dDb78ay3A7CiKIoJSSlJS8kltJMv7t7m//RtDt6BbnQbGsahracpzK40+/FUglAUpU3IyyynvLDWaktrGKvPjY2D535bkW72Y6kEoShKm5CWkofWUUNMT+up3Ho1XNwd6XNjJCcOFnPiYJFZj6UShKIodk+vN5C+M4/IbgE4udp+02u368LwCnBh6/JjGAzmK8GhEoSiKHYv+1AJNRUNNjX24VK0jhoGTIqhKKeSI9vMN/OcShCKcgmVJXXUVjZYOgylhdJScnF2cyAiwd/SoZhMbO8ggqO8SF6VgUFnnqsIlSDs1OHDhxk4cCDOzs68+eablg7HJhWdqmTxy8ksmb2dqjLLTdqitExDnZ6MPYXE9ApC62g/b3lCCAZPiaWqrJ5C05aZO8t+zpZyHj8/P95//32eeuqpK9pOSolBb7j8inauoriWtXP2oHXQUFPVwNq5e6ivsez8wMrVydxTgK5Obze3l87VLtaHmJ6BVOVJs5QDVwnCjLKysoiPj+fuu++mY8eO3Hbbbfz0008MHjyYuLg4UlJSAHjxxRfP+5TftWvXKy7kd6GgoCD69u2Lo6Oj0dtIKSkvqKEwu5K66rZ7W6W2qoE176dSX6Nj/MxERj/YleKcKr77aB96nUqetiYtJQ8PX2fax/pYOhSzGHZnZyKHC7MUHrT95nwjbVqSRuFJ0w4sCQj34NqbL10WIz09naVLl7JgwQL69u3LokWL2Lx5M6tXr2b27NmsXNnsVBfNupJifVdKGiRlhTXU1+jQaDWUFdbiEyjsosfHldDV6/l23l7KCmuY8GgiAWEeBIR5MOyOeH7+/BC/LDzEiLu7IDTWXwVUgZqKek4cLCZxRLjd/s6cXR3MVpW2bf33W4C1l/sGMBgarxzqa3V4+rng7OZASV41ZQU1eAe5tlrlSEsz6A388MkBcjPLGHV/V0I7/T4VZfzAdlSV1bFtZQbuPs4Musn2avm0Rek785EGafOD4yylbfznw2U/6ZuLpcp9G8tgkJTlV9NQp8fT3wVXDycAfILdKM2tpiy/Bp9gNxydtVe1f1shpWTDoiNk7S1kyK0die39x6lXe42KoLKkjt0/nsDdx5kew8Ob2ZNiTdJScvFr705AmIelQ7FJbSZBWDNzlPs2xrnJwSvAFRf339srtFoNPsFulORWU5pfjW+wGw5O9pskUtZmcnDLaXqPiaDb0LBm1xFCcO0tHakqrWPz0qO4ezs3m0gU61BWUENuRjkDJrVOaWx7pBqprcCUKVMoLi6mX79+zJ071yTlvnNzcwkLC+Ptt9/mX//6F2FhYZSXl59dbtAbKM1rPjmcoXXQ4BPsihBQml+NrkHf4ris0f6NOez4NovOg9rRf8Kl30w0GsHI+xIIifLmp08PcupoSStFqVypo9tzAdut3GoN1BWEGVmy3HdISAjZ2dnNLjPoDZTm16Br0OMd6Iqz28V7Ojk4avEJcqMkr5rSvBp8Q9zQOtjP54pju/PZ8NURIrv5M/S2TkY19jk4aRn7cHdWvLmTdR/uY/JTvfBvr25hWJPGyq15tIv1xsvf1dLh2Cz7+U9XjKLXGyjJq0bXYMA70O2SyeEMByctPsFuSCkpzau2m66eOWklrP/vQUKivBj5QNdmJ4O/GBcPR8Y92gOto4a1c/ZQWdJ8u5FiGYUnKynJrVaN0y2kEkQbotcZKM2txqCT+AS64nwFXVgdnbT4BLli0EtK86ttfjBdUU4l6z7ch1eAC2Mf7oHjVbSvePm7Mv7RHtTV6FgzZ0+bHjtibdJSctFohWojaiG7TxDmGF1oi/S6xjYHg142dl29ivENjs4OeAe5otdJSvNr0Otss02ivKiGNe+n4uikYfzMRFw8jB9MeKGAME/G/LkbpXnVrPtwH/oG206c9sBgkBzdnkeHBP9m29YU49l1gnBxcaGoqKjNJwldg4GS3GoMBolPcMvGNTi5OOAd6EpDnY6Tmbk4OdnWzFy1lQ2seX8PDfUGxs9MxNPPpcX7DI/34/q7OnPqaCk/fXYQacbyy8rlnUoroaqs3i5La7Q2u26kDgsLIzs7m4KCAkuHYpTa2lpcXFr+hnUug95AdXkDEombpxOFlab5TKCr11N4vIaKExoiHtTbRBfYhno9a+ftoaKolgmP9cA/1HQNyx37hVBVWs/WFem4+zhzzbQ4k+1buTJpKXk4umiJ6h5g6VBsnl0nCEdHR6KioiwdhtGSkpLo2bOnyfZXfKqKVe/uRkrJhMd6mnyw0JHaXH767iDfz9/PmD93s+reTQa9gR8/3k9+VjmjHuxK+zjfy290hRJvCKeytJY9P5/Ew9eZxBEdTH4M5dJ0DXqO7conJjHQJj60WLvL/keLRrcLIf7R9LiDEKKf+UNTWqIwu5KV7+wCYNLjvcwykrRT/xCG/qkTx/cXsX7BAattuJZSkvTlEbL2FTFkeidiepqn4VIIwTVT44jtHcSWZemkNfXDV1rP8X1F1NfqVe8lEzHmI98HwEBgetPjCmCe2SJSWiz/eDkr39mF1kHD5Cd74dfe3WzHSrg2lGumxXFsVwG/fHHYKu+/J6/O4NDW0/QZG0nXIaFmPZbQCK6/uzPt43z4+bNDZB8uNuvxlPOlpeTh5uVEaLzprxDbImMSRH8p5SNALYCUsgRwMmtUylXLzShj1bupOLk4MPnJXvgEu5n9mD2uD6f/hCiObMtlw+I0q+oUsPfXbHZ+d5wu17Sn37jWud3o4Kjlxoe64RPsxnf/2UdhdkWrHLetq61qIGt/IXF9gtHYaeXW1mZMgmgQQmgBCSCECASs815CG3fqaCmr30vFxcORyU/2wiug9UaQ9h4TSa9RHTiwMYety9OtIkmk78xn05I0IrsHcN30jmYridwcZzdHxj/aAydXB9bM2UN5UU2rHbutythdgEEn6dhf9V4yFWMSxPvAN0CQEOIVYDMw26xRKVfs5OFi1sxJxcPXmZue7GWS7ptXQgjBgEkxdBsaRupPJ9n+bVarHv9COUdKWP/pAUKivBl1f8IVjZI2FQ9fF8bN6IG+wcDaOXuorVID6cwpLSUXn2A3AjuYplyNYkSCkFJ+CTwNvAqcBiZJKZeaOzDFeMcPFPHtvL14Bbgy6YleuPtYZmyCEIJrb44jflA7tq/NZPePJywSR2F2Bes+3It3oBtjH+lu0d4s/qEe3PhQN8oKa1j3wV509bY5uNDaVZbUknO0lI79glv1StHeGdOLqQNQDawBVgNVTc8pViBzTwHrPtyLb4gbk57oiZuXZZuHhEYw7PZ4YvsEsXVFOvuSmi8YaC7lhTWseX8PTq4OjH+0h1WMpG0f58sN9yRwOqOM9QsOYrDChnxbl7Y9D6Sq3Gpqxlx3fwusbfr+M5ABfHe5jYQQC4QQ+UKI/RdZLoQQ7wsh0oUQe4UQvc5ZdpcQ4mjT113GvZS2J31nPt9/tJ+AME8mzup5drIfS9NoBCPu6UJk9wA2Lk7j8G+nW+W4NZX1rJmzB73OwLhHe7T6bbZLie0dxDXT4shILWDT19bVkG8P0lLyCI7ywifI/J0y2hJjbjF1k1J2b/oeB/QDfjNi358Boy+xfAwQ1/T1IPAhgBDCD3gB6N90rBeEEKrP2gXSUnL58ZP9BEd5MfGxRKv4pHwurVbDqAcSCIv35ZeFh0jfmW/W4zXU6Vk7dy8VxbXc+HB3qyy/3WN4OD1v6MD+DTns+uG4pcOxG0WnKinKrlSlNczgilvupJS7aHzzvtx6G4FLdQKfCCyUjbYBPkKIdsAoYL2UsripS+16Lp1o2pxDW0+x/tODtI/zYVxTTxlr1NjdszshMd6s/+8BsvYVmuU4er2B7+fvp+B4OSPvS6B9rI9ZjmMKAyfHENc3mG0rMzi8rXWurOxdWkoeQiOI7a0ShKmJy13qCiGeOOehBugF+EspR11250JEAmullF2bWbYWeE1Kubnp8c/AX4GhgIuU8l9Nz/8dqJFSvtnMPh6k8eqD4ODg3osXL75cSFatsrISD49Lf/ItTpec3iFxD4EO1wg0DtbfIKevl2QlSepKocN1Ao9g42I25nxIKTmVLCnNgnZ9BH6x1n8+DHrJiY2SqnyIGCLwaGe689FWnDkXUkqOrpE4e0HEUOst9WJuLfnbGDZs2E4pZZ/mlhnz0fPcPmM6Gtsill9VJCYmpZwPzAfo06ePHDp0qGUDaqGkpCQu9Rr2/HySAzuOEtHNn9EPdsXB0XZqzdQOauCbt3eRs7WWCTMTaRfjfdltLnc+AH775hilWcfpOy6q1QbCmUL9IB0r3trFqW01TH6yl1FdM405H23FmXNxKr2Ug9W7uO7mznQa0M7SYVmMuf42jGmD+Oc5X69IKb+UUppi+qwcIPycx2FNz13s+TZt1w/H2bz0KNGJgYz5v242lRygcQa2ibN64uHjzNo5qRScaPno4j2/nGTXD8dJuLY9fcdGtjzIVuTk6sD4GY29rNbMSaWsQA2kuxppKXk4OGqISgy0dCh26aIJQgixRgix+mJfJjj2auDOpt5MA4AyKeVp4AdgpBDCt6lxemTTc23W9m8z+e2bY8T1CWLkAwlWXTX1Uty8nJjwWCLObo6sfi+VopzKq97X0R15bF56lKgeAQyZbtxc0tbG3ceZ8TN7YDBI1sxJpaay3tIh2RS9zkD6zjyiegS0aI4T5eIudVb/cM//SgghvqKxPSFACJFNY88kRwAp5X+AdcCNQDqN4yzuaVpWLIR4GdjetKuXpJRtsuKZlJLk1Rns/O448QNCGHZnZ5uvMePp58LExxP55s1drH4v9arqRWUfLuanzw7SLsabkfcl2PQ58Q1xZ+xD3Vn1XirfztvLxMd7XtX0p23RiYPF1FXpVOVWM7pogpBSbmjJjqWU0y+zXAKPXGTZAmBBS45v66SUbF2eTupPJ+lyTXuG/qkTwobfCM/lHejGhFk9+eatXax6dzeTn+qFl79xdaMKTlSw7j/78Aly48aHLDtK2lTaxfow8r4Evv9oHz9+coAx/9fVIqVBbE1aSi4u7o6EJ/hZOhS7ZcxI6jghxDIhxEEhRMaZr9YIrq2SBsmmr4+S+tNJug0Ns6vkcIZfO3cmPJZIQ0e9NSAAACAASURBVJ2e1e+mUlVWd9ltygpqWDN3D85WNEraVKITAxkyvRNZewvZ8JUaSHc5+gZJ1p5CYnsHoVXJ1GyMObOf0jiITQcMAxYC/zNnUG2ZNEiSFh1hX1I2iSPCufaWOLtLDmcEhnsybkYPqsvrWfXupe/BV5fXs+b9VAy6xrmkPXytZ5S0qXQdEkrvMREc3HyKHeuyLB2OVavIbpxrXQ2OMy9jEoSrlPJnGsdMHJdSvgiMNW9YbZM0SH5eeIiDm0/Re0wEg6bE2mTj65UIifZm7MPdKS+sYfV7qdRV/7HiaX2tjm/n7aGqtI6xj/TAr535JkCytP4TookfEELKmkwObjll6XCsVulxiae/CyFGdJdWrp4xCaJOCKEBjgohZgghJgNqtI6J1VY1kL1NcmRbLv3GRzFgYozdJ4czQjv5Mub/ulF8qoq1c/dQX6s7u0yvM/DD/P0UnKhg5ANdjRo/YcuEEAy9I54OXfyapkk1z+hzW1ZVVkdVHnTsqyq3mpsxCeIxwA2YCfQGbgdUAT0TODPB+roP9/Lp05spP9FYiqHvWNsZ8GUqEV39GXl/AnlZjaW6dfV6pJT88sUhThwsZuht8UR1D7B0mK1Cq9Uw6sGuBIR58MPH+8nLLLd0SFYjJ62Eb95snGu90wDVe8ncjOk8rJdSVgKVNHVFVa6eNEhOHyvlSHIex3blU1etw83LiW7DwqjQZtNrVISlQ7SYmJ5BXH+XgZ8+O8j38/dT2SApOpJH/wlRdLmmvaXDa1VOLg6Mm9GD5W/sYO28PUx5urelQ7Ko+hodW785xoGNOXgFuBAxVOAbYr+3Gq2FMQniLSFECLAM+FpK2Wz5buXSik9XkZacS1pKHhXFtTg4a4lJDKRj/2DC4v3QaARJSW1+wDid+oegq9eT9OURALpeF0rvMZGWDcpC3LycGP9oIsv/vZM176cSMqht9mzK2lfIhkVHqCyta5r/PJotv22ydFhtwmUThJRyWFOCuBn4SAjhRWOi+JfZo7NxVWV1pO/I50hyLgUnKhACwrv40X9iNNGJgTg6234ffnNIuDYUoRHs3X6Ya29p3bmkrY1PsBvjHunBynd2kb5O4qfJosf14TZXauVq1FY2sGlpGmnJefi2c2fKX7oSEm3fbVDWxqjx6VLKXOB9IcSvNE4/+g9AJYhmNNTpyUgtIC05l5OHipESAjt4cs20OGL7BOHubZnpQG1Nl8HtyW9Is+lR0qYSHOXFLc/1Y/X8bWxbmcGBjacYODmG2D5Bdpk8pZSk78xn09dp1FXp6HNjJH3GRKJ1VOMdWttlE4QQojNwCzAFKAK+Bp40c1w2xaA3kH2khCPJuWSkFqKr0+Pp50KvURF07B9i190yldbhE+xGh2s1xIZ0Z8vydH787wH2/HKSa6bF2dWn6qqyOjYsOkLmnkICO3gy4bF4AsIuX+lWMQ9jriAWAIuBUVJK1TG7iZSSwpOVHEnO5ej2PKrL63FydaBj32A69Q+hXYy33Q5wUywnLN6PaX/ry5Ftp9m2KoPlb+wktk8QAyfF4BVgXLkSaySl5PBvp9myLB1dvYGBk2NIHBGuSo5YmDFtEANbIxBbUVFcS1pKLkeS8yg5XYVGK4jo6k+nASFEdPVvE/eGFcvSaASdB7UnplcQu9efIPXHE2SmFtLj+jB6j4602hkGL6a8sIakLw9z8lAJ7WK9GX5H5ysu4KiYh239JVlIXXUDx3YVcCQ5l1NHSwFoF+vNdX/qRGzvILuqCaTYDicXB/qPjybhmvZsW5XBrh9OcGjrafqNj6bL4HZW/+lbGiT7NmTz28oMBDDk1o50HRKqrrytiEoQF6HXGTi+v4i0lFyy9hah1xnwCXaj/4Qo4vqG4B1ou5fzin3x8HVhxN1d6D4sjM1Lj7KhqZbX4CmxdEjwt3R4zSrJreLXLw5z+lgZHbr4cd1tnYyu6Ku0HmMaqadJKZde7jl7IKUkN6OctORcju7Mo65Kh6unIwnXtqdj/xCCIjztsteIYh+CIryY/GQvMlIL2LriGGvm7KFDgh+DpsTi3946quPo9QZS159g+9osHJw0XH93Zzr1D1H/V1bKmCuIvwEXJoPmnrNZpXnVHEnJJS05l/LC2rNTGHbsF0x4Fz9VTlixGUIIYnoGEdk1gH0bstn+bRZfv5xCl2tD6TcuCjcvJ4vFVnCigl++OEThyUpiegZy7a0dVbdvK3fRBCGEGEPjjG+hQoj3z1nkRWPpb7tQVlDNly9sAwFhnXzpOy6K6MRANYWhYtO0jhoSR3Sg04AQtn+bxf4NOaSl5NJnTCTdh4e1amcKXYOeHd9msevHE7h4ODL6wa7E9ApqteMrV+9S74KngB3ABGDnOc9XAI+bM6jW5B3oxvA74wnv7I+Hr/o0o9gXVw8nhtzSkW7XhbJ1xTF+++YY+zfkMPCmGGJ7m3+g3eljZfz6xSFKcquJHxDC4GlxqlOHDbnUlKN7gD1CiEVSygYAIYQvEC6lLGmtAFtD50FtqxCc0vb4hrgz9uHunDxczJZl6fz4yQH2/nKSwVPNM9CuvlZH8qoM9iZl4+HrzLhHexBhpQ3mysUZcx9lvRBiQtO6O4F8IcRWKaXdXEUoSlsRHu/Hzc/25fBvp0luGmgX1zeYAZOiTdaL6OShYn7932Eqimrpdl0oAybHqFu2NsqY35q3lLJcCHE/sFBK+YIQYq+5A1MUxTw0GkGXwe2J7R3E7h9PsHv9CTJ2F9BjRDi9R0Vc9UC7uuoGtixL59DW03gHuTL5yV60j/MxcfRKazLmL8FBCNGOxmquz5k5HkVRWomTiwP9J0TT5Zr2bFt1jF3fH+fQllP0nxBN50FXNtAuI7WADV8doaa8np4jO9BvXBQOTqqqgK0zJkG8BPwAbJFSbhdCRANHzRuWoiitxdPPhRvuSaD7sHC2LDtK0pdH2PtrNoOnxtKhy6XbDarL69m0JI30Hfn4h3ow9uHuBEV4tVLkirkZU4tpKeeMeZBSZtBY2VVRFDsSHNk00G53AVtXpLPm/T10SPBn8JRY/NqfX5FYSklaSh6blxylvk5H/wlR9BwZgdZBjRmyJ8aMpA4D5gCDm57aBDwmpcw2Z2CKorQ+IQQxvYKI7BbA3qRsdqzLYvG/Uki4pj39xkfh6ulEZUktSYuOcHxfEcFRXgy/o/MfEohiH4y5xfQpsAiY1vT49qbnbjBXUIqiWJbWUUPPGzoQPzCE7Wuz2L+xcaBdx/4hpCXnYtBLBk+NpfvwcDWpkx0zJkEESik/PefxZ0KIWeYKSFEU6+Hq4dRYZfW6ULauSGf/hhxCO/ky7PZ4VbCyDTAmQRQJIW4Hvmp6PJ3GmeUURWkj/Nq5M+6RHpQX1uDp76KK67URxrQo3UtjF9dc4DQwFbjHnEEpimKdvAJcVXJoQ4zpxXScxnpMiqIoShty2SsIIcTnQgifcx77CiEWGLNzIcRoIcQRIUS6EOKZZpZHCCF+FkLsFUIkNfWYOrNML4RIbfpabewLUhRFUUzDmDaI7lLK0jMPpJQlQoiel9tICKEF5tHY2ykb2C6EWC2lPHjOam/SWL7jcyHEcOBV4I6mZTVSykRjX4iiKIpiWsa0QWiaqrgCIITww7jE0g9Il1JmSCnrgcXAxAvW6QL80vTzr80sVxRFUSzEmDf6t4DfhBBnRlNPA14xYrtQ4OQ5j7OB/hesswe4CXgPmAx4CiH8pZRFgIsQYgeNkxO9JqVceeEBhBAPAg8CBAcHk5SUZERY1quystLmX4MpqfNxPnU+fqfOxfnMdT6MaaRe2PRGPbzpqZsuuE3UEk8Bc4UQdwMbgRxA37QsQkqZ01T76RchxD4p5bELYpsPzAfo06ePHDp0qInCsoykpCRs/TWYkjofjSUtalJTqfhxPcc8PRj08MOWDskqqL+N85nrfBhV17cpIVxpUsgBws95HNb03Ln7PUXjFQRCCA9gypn2DillTtP3DCFEEtATOC9BKIq90peVUbZqNaVLl1J3tLE2po8QFDo44v/A/aqrqdIqzFlZazsQJ4SIEkI4AbcC5/VGEkIECCHOxPA3YEHT875CCOcz69BYB8pUVy2KYpWklFTv2MGpv/6Vo0OuI2/2bISLCyEvv0Tcls3U9e5Nwdtvc+rJJzHU1Fg6XKUNMNs0T1JKnRBiBo2lwrXAAinlASHES8AOKeVqYCjwqhBC0niL6ZGmzTsDHwkhDDQmsddMeFtLUayKrqSEslWrKF2ylPqMDDQeHvhMuQmfadNw6dz57Hpl991Lh6HXkf/W29RlZhE+dw6OoaEWjFyxd2adB1BKuQ5Yd8Fz/zjn52XAsma22wp0M2dsimJJUkqqU7ZTumQJFT/+iGxowLVHD9q98gpeY0ajcXP740ZC4H///TjHxZHz1F/InDqN0Pfexb1fv9Z/AUqboCaKVZRWpCsupuyblZQuXUp9VhYaT098brml8WqhU0ej9uFx3XVELvma7EdmcOLe+wh+9m/4Tp+u2iVaWeWmTVQmbSBw1mNoPT0tHY5ZqAShKGYmDQaqk5MpWbKEip9+hoYGXHv3pt2f/w+vUaPQuF55VVTnqCgiv17Mqb88Td5LL1N36BDBf/87GicnM7wC5UK1R9LInvkYsqaGqs2bCZs7B+e4OEuHZXJq+idFMRNdQQGF8z/m2KjRnLjnXqq3/obfn6YTvXYNkV/+D59Jk64qOZyh9fQk7IN5+P/5/yhduowTd92NrqDAhK9AaY6+tJTsGTPQengQ+u676KuryLzlVsq/+87SoZmcuoJQFBOSBgNVW7ZSunQpFb/8Ajodbn37EjhzJp4jb0Dj7GzS4wmNhqBZs3CJj+fU354lc+o0wubOwbWbasIzB6nTkfPEk+hyc4n4YiGuiYm49uxJzqxZ5Dz+BDX79hP0xOMIB/t4a7WPV6EoFtaQl0/ZNysoXbqMhpwctL6++N1xBz7TpuEcHWX243uNHo1TZCTZDz/C8dtup93LL+E9UVWuMbX8d96hautWQl5+CdfExlJxjsFBRHz+GXmvvUbxggXUHjhA6Dtv4+DnZ+FoW04lCEW5SlKvp2rzZkqWLKUyKQn0etwGDCDoySfwGDGi1dsDXOLjiVy+jJxZj3Pqr89Qe+gwQU89aTefZi2tbO23FP93Ab5/mo7vtGnnLRNOToT84x+4dOtO7osvkjllKmHvv2fzV3LqL0dRrlDD6dOULl9B6fLl6E6fRuvvj/+99+AzdSpOEREWjc3B15cOn3xM3utvUPzZZ9SlpRH69ltofXwuv7FyUbUHD3L6+edx7dOb4Gf+MHPBWT6TJ+HcMY6cR2dy/E+3EfLCP/CZOrUVIzUtlSAUxQhSp6Ny4yZKlyyhcuNGMBhwHzyY4GeewXPYUIQV9R4Sjo6EPP8cLp3jyX3xn2ROu5mweXNx6WhcN1rlfLriYk7OmIHWx4ewd9+97O/aNSGByOXLOPXkU5x+/u/U7N1H8PPP2WQPM5UgFOUSGk6fpnTpUkqXLUeXn482MAD/Bx7AZ+oUnMLDL78DC/KZMgWn6GiyZ84k69bptH/9NbxuuMHSYdkU2dBAzuNPoC8sIuLLL3EICDBqOwdfX8I/nk/Be+9TNH8+tYcPE/beuzi2a2fmiE1LdXNVlGboSkrIe/VVjo0cReGH/8E5vhNhc+cQ98svBD0+y+qTwxluPXsStWwZzrGx5Dw6k4K585AGg6XDshl5//431cnJhLz0T1y7db2ibYVWS9ATjxP6/nvUp6eTOWUqVckpZorUPFSCUJRzGKqqKPjgA46NuIHiL/6H14TxxKxfT4f58/EcMQLh6GjpEK+YY3AwEV8sxHvSJArnziV75kz0lVWWDsvqlX6zkpKFX+B31534TJp01fvxGjmSyGVL0fr4cOLeeyn69DOklCaM1HxUglAUGm8lFC9aRPqo0RS+Pwe3gQOIXr2K9q+8glOY7RfE0zg70+7V2QT/7Rkqf03i+PRbqT9xwtJhWa2affvIfeGFxl5pf/lLi/fnHB1N5JIleA4fTv7rrzdW5K2uNkGk5qUShNKmSYOBsm+/5djYceS99DLOkZFEfLWI8LlzcY6NtXR4JiWEwO+uu+jwycfo8gvInHYzlVu2WDosq6MrLCR7xqM4BAQQ+s7bJusmrPVwJ/T99wh88gnKv/+BrFtupT4ryyT7NheVIJQ2q3LLFrKmTuPUk0+hcXEh/KP/0OGLhbj17Gnp0MzKfeBAIpctxTEoiJMPPEjRZ7Zzy8PcZH092Y/NQl9WRti8uTj4+pp0/0IIAh54gPCP56MraEzSFb/+atJjmJJKEEqbU7NvP8fvuYeT992PvrSU9q+/RtQ3K/C47ro2UxHVKTycyMVf4Xn99eS/9jqnn3kGQ22tpcOyuLzXXqNm507avfKv8+biMDWPwYOJXLYMp/Bwsh96mIL351hl5wGVIJQ2oz4ri+xZj5M1bRp1hw4T/OzfiP7+O7wnTkRotZYOr9Vp3N0Jfe9dAmY+Stmq1Ry//Q4acnMtHZbFlCxdSsmir/C77168x441+/GcwkKJWPQl3pMnU/jBB5x86CH0ZWVmP+6VUAlCsXsN+fmcfvFFjo0dR+XGjQQ8/BAxP63H7847bXLwkikJjYbAhx8mbN5c6jMyyJw6jepduy0dVqur3r2bvJdexn3wYIKeeKLVjqtxcaHd7FcIefEFqrb+RubUadQeOdJqx78clSAUu6WvqCD/nXc5Nmo0pcuW43vLLcT++AOBM2ei9fCwdHhWxfP664n8ejEadzeO33UXJUuWWDqkVtOQl0/OzMdwCAkh9K03W/1qUgiB7623ErHwc2RdHVm33ErZmrWtGsPFqASh2B1DXR1FCz7l2IgbKProIzyHDSPm27WE/OPvRo+EbYuc4+KIWrIE9379yP3HC+S+9DKyocHSYZmVob6enJkz0VdVETZ3rkVrVrn17EnU8mW4dE3g1F/+Qt6rr1r8/KsEodgNqddTuuIbjo0eQ/4bb+DStSuRy5cR+vZbFi+iZyu03t6Ez/8Iv/vupWTRIk7cex+64mJLh2UWUkpyX3qJmj17aP/qq0ZP+WpODoGBRHz6Kb533kHx5ws5cc+96AoLLRaPShCKzZNSUvHLL2ROmsTpZ5/Fwd+fDp99Sof/foJrQoKlw7M5Qqsl+C9/of2/36Bm714yp06l9tAhS4dlcqWLF1O2bDn+f/4/vEaNtHQ4ZwlHR0Kefbbx/O/fT+ZNU6hJTbVILCpBKDateudOjt92O9kPP4KsbyD03XeIXLoE9wEDLB2azfMeP56IL78EgyRr+p8oX7fO0iGZTPWOHeS+MhuP664j8NFHLR1Os7zHjydy8VcIZ2ey7riTksWLW328SptPEFJK8t98k4qff0ZfXm7pcBQj1aalcfKhhzl+2+00nDxJyIsvEr12DV6jR7eZsQytwbVrAlHLluLSpQs5TzxJ/ltvI3U6S4fVIg2nT5P92CycwsJo/+83rLqLs0t8PFHLluI+cAC5L/6T0889j6GurtWO3+bLfTfknKL4f18iP/kvaDS4dOmC+4D+uPUfgFvvXmjc3CwdonKOhpwcCubMpWzVKjTu7gTOmoXfnXeo35MZOQQEEPHZp+T+6xWKPv6Yyg0bCH72WdwH9Ld0aFfMUFtL9qMzkbW1hC38HK2Xl6VDuiyttzfhH35I4bx5FH7wIXVHjhD2/ns4hpq/RlibTxBOYaF0TEmmdu9eqrYlU71tG0WfL6Tok/+CoyOu3bvj3r8/bgP645qY2Ob7zVuKrqSEoo/mU/LllyAEfnffjf+DD5i8FILSPOHkRLuX/on74MHkv/46J+6+G88bbiDor0/jFBZm6fCMIqUk98V/Urt/P2EfzMM5JsbSIRlNaLUEzpyJS9dunHr6aTKnTCX07bdwHzTIrMdt8wkCQOPkhFufPrj16QMzHsFQU0P1rl1Ub0umKjmZwv/8Bz74AOHsjFvvXrj1H4D7gP64JCSo+X7NzFBdTfHCxoRtqK7Ge9IkAmc8gmP79pYOrU3yGjUSj+uGUPzZZxR+NJ/KDRvwu/ceAh54AI27u6XDu6SSL/5H2cqVBMyYgefw4ZYO56p4Dh9G1LKlZD/6KCfuf4CgJx7H7777zHY89e7WDI2rKx6DB+MxeDDQOOCqevsOqpO3UbUtmYJ33qGAxlIFbn374jagP+4DBuDcsSNC0+abdUxC1tfjunEj6X//O/qCQjyGDyfo8Vk4x8VZOrQ2T+PiQsCf/4z35Mnkv/kWRf/5iLIV3xD01JN4jR9vlW1AVduSyXv9dTyuv56Ahx+ydDgt4hQZSeTixZx6/nny33yLmr37EDeOMcuxVIIwgtbTE8/hw/AcPgxonKO2OiWFqm3bqN6WTGVSUuN6Pj649e9/tg3DKSrSKv9ZrFVDXj5VmzdRuWEjVVu34lVZiVPv3gS99x5uvXpZOjzlAo7BwYT++w18p08nb/ZsTj39V0oWfUXwc8/i2q2bpcM7qz47h5xZs3CKjKT966/ZxYc4jbs7oW+/TXG37uS/9Ra+hw8jR440+WtTCeIqOPj54TV6NF6jRwPQkJtLdXIyVduSqdq2jYoffmhcLyio8eqi6ZZUazQq2RKp01GTmkrlho1UbtpE3eHDQON58xw9iqx27Yh/+GGVZK2cW6+eRC75mrKVq8h/+22ypt2M9+TJBD4+C8egIIvGZqipIfvRR5F6PWFz59hViRUhBP733oNLly7s3bLZLIlPJQgTcAwJwXviRLwnTkRKScPJk2evLqq2bKV89ZrG9cLDz15duPfvh0NgoIUjb30N+flUbdpM5cbGqwRDRQVotbj17Engk0/gMWRI4606IUhLSlLJwUYIjQafmybjObKxvEnxZ59T8cMP+D/0Z/zuussinTuklJx+/u/UHT5M+H8+xDkqqtVjaA3uA/pTV1tjln2bNUEIIUYD7wFa4BMp5WsXLI8AFgCBQDFwu5Qyu2nZXcDzTav+S0r5uTljNRUhBE4dOuDUoQO+N9+MlJL69PTGq4vkbZT/8COlS5cB4BQTc7aHlHu/fhaO3DykTkfNnj1UbtxE5caN1DWNyHUIDMRz1Eg8rh2C+6CBaD09LRypYgpaDw+CnnwSn6lTyXv9DQreepvSpcsI/uvTeAwf3qoJv3jBp5R/+y2Bjz+Ox3XXtdpx7YnZEoQQQgvMA24AsoHtQojVUsqD56z2JrBQSvm5EGI48CpwhxDCD3gB6ANIYGfTtiXmitdchBA4x8XhHBeH3x23I/V6ag8dPtvgXbpyJSWLFjV23QwN5dR33+EUHYNTdBTOMTE4hYcjHB0t/TKuiK6ggMpNm6nctJGqLVsxlJeDVotrz0QCn3gCjyHX4typk7o6sGNOERGEfzCPys1byHv1VbIfmYH7oEEE/+2ZVuloULl5C/lvvYXnqFH4P/iA2Y9nr8x5BdEPSJdSZgAIIRYDE4FzE0QX4Ezx9V+BlU0/jwLWSymLm7ZdD4wGvjJjvK1CaLW4dk3AtWsC/vfdh6yvp2b/fqq2bSNn/U9UbUumbNXq3zdwcMCpQwecY6Jxiopu/B4dg1NUFFoP6+hWKHU6avbubbxttHETtQcbf8UOgYF4jhiBx5CmqwQbGJSkmJbHNYNxX/kNJYu/pmDOHDImTcZ3+nQCZzxitsqp9SdOkPPkkzjHxtJ+9ivqg0gLCHPV9hBCTAVGSynvb3p8B9BfSjnjnHUWAclSyveEEDcBy4EA4B7ARUr5r6b1/g7USCnfvOAYDwIPAgQHB/devHixWV5La6msrMTDwwNRW4s2NxeHpi/t6abvBQWIc6Yl1Pv6ogsORt8uBF1I45c+JASDlxeY+Z9CU16O04EDOB84gNPBQ2iqq5EaDQ3RUdQlJFDftSu6sLAWxXHmfCiNbP18iMpKPFavwXXTJqSbG5Xjx1Nz7TVwFaUuLnYuRG0tvm/8G21pKcV/ewZ9G2nna8nfxrBhw3ZKKfs0t8zSjdRPAXOFEHcDG4EcQG/sxlLK+cB8gD59+sihQ4eaIcTWk5SUxKVeg6yvp/7kSeoyMqg/lkF9ZgZ1xzKoT07BUF19dj2NlxfOUVE4xcTgHB2FU3QMzjHROIaGXvXAPqnXU7Nnb+Nto42bqD1wAABtYAAeo0bhcd0Q3AcOROvtfVX7b87lzkdbYxfnY9w4ao8cIe+V2WgWLyZw506Cn3v2iosrNncupJTkzHqcitxcwj+eT0LTOKa2wFx/G+ZMEDlA+DmPw5qeO0tKeQq4CUAI4QFMkVKWCiFygKEXbJtkxlhtgnBywjkmprFEwA2/Py+lRJeXR92xY9RnZFKXcYz6YxlUbtpI2YoVv2/v6IhTZCRO0b/fqnKOjsIpKgqNq+sfjqcrKqJy0yaqNm6iasuWxvlyNRpcExMJnPVYY4+j+Hi76FeutB6XTp3o8PlnVKxfT/7rb3Di7ntMUrajaP7HVPzwA0FPP312kKvSMuZMENuBOCFEFI2J4VbgT+euIIQIAIqllAbgbzT2aAL4AZgthDhTaGdk03KlGUIIHENCcAwJgQv+MfRlZdRnZlJ3LKMxcWRkUnv4EBXr18M5t6sc27c/e8UhXFyp2rKF2v37AdAGBOAxbBgeQ67FfdAgi866pdgHIQReI0fiMeSCsh333EPAg1detqNywwYK3n0Xr3Hj8LvnbvME3QaZLUFIKXVCiBk0vtlrgQVSygNCiJeAHVLK1TReJbwqhJA03mJ6pGnbYiHEyzQmGYCXzjRYK1dG6+2Na2IiromJ5z1vqK+nPiuL+oyMs7es6jIzqN6+vbHMRY8eBD42E/drh+DSpbO6SlDM4ryyHW+9RdFHH1H2TVPZjnHjjPq7q8vMJOepv+AcH0+7l19SjdImZNY2CCnlOmDdBc/945yflwHLLrLtAn6/olBMTOPkhEvHjrh0PH+aRWkwIOvqmr3lpCjm4hgcTOgbORLDgAAAHHRJREFUZ8p2vNpYtuPLRY1lO7p3v+h2+spKsmc8inBwIHzuHPV3a2LqY6FyHqHRqH8yxWLcevYk8uvFtJs9m/qcHLJuvoVTz/yNhvz8P6wrDQZO/fUZ6rOyCH3nHVXKxgxUglAUxaqcKdsR8/13+D9wP+XffkvG6DEUzv/4vNnUCj/4kMqffyb4r3+1ycmLbIFKEIqiWKUzZTui167BbeBACt5+m4xx46n4+WecU/dQOHcu3pMm4XvH7ZYO1W5ZehyEoijKJTlFRBA+by6VW34v2+EtBC7duhHyzxdVo7QZqSsIRVFsgsfgwUR/8w3Bzz1HfXw8YXPeR+PsbOmw7Jq6glAUxWYIR0f87ridveFhjeN+FLNSVxCKoihKs1SCUBRFUZqlEoSiKIrSLJUgFEVRlGapBKFYJ30DDg0VYKb5ShRFuTzVi0mxLmU5sPMz2PU511TmwXZP8I0An4jmvztZx6x6ShtkMIChARws39XWXBO/qQShWJ6UkJEE2z+BI9+BNEDHURzThRAT6AIlx6EkEzJ+hYbq87d1C2hMFL6Rf0we3uGgta35vBUrZDBA6XEoONz0daTpexroaiCwM4T2hPY9oX0vCE64qqQh5f+3d+7BcV33ff/89t59AfsCQAAESIFP8CmZAEVSlCzRlC17lEqp5UzGdhPbqh2P606dpG5aj5rJtNP85UzStO609Yxiu1ZdjRTH8kPNKJYa23QUW6RE8SUSEClRpPjCG1jsLrDvPf3j3N29CyxIiiQeFM5n5s6553HvPTg4e773/M655yjShTST2UkSuQSJXKJ6nk0wmZskkZ0R7py3SRsP8uAt/9ONQBgWj3Qcjj+jhWHsbWhogft+H3Z9HprWcvHAATa4d8lSCqZG9Y914rzjvqvdy69D30+gVKimFw9EVlUFY6aIhNphCS5jni/miWfjTGQniGe0O5md5O3k22TPZwl7w4R8IULeUMUN2kHzRfHNUirqeuUWguF+GH1LC0GZcCe0boa7HwdvAwwc1y82R/8PAFnLx2T7VhJtW0i0rGMy1knCHyZRmJqz8S+7BVWonzfAEouIL0LUHyXiixALxFgTWUPEF6EwPPd1N4MRCMPCM3Bci8KJv9E/vNV74BNPwraPgzcw93UiEGrVx+o6W+gWC5C8UhWNinse3v4ZpAZr01t+iHXV9jrcIhJsmv2M90hJlUhkE7qxz8YZz4xXGv2Km40Tzzhx2TipfGrO+z3zy2fqhlti0ehtJOwLV4Qj7A3T6Gsk5A1Vw8txjr9yjS9Eo92I5Xnv+0PfdhQLjhD0u4TgTRg9A8XqYoBEVmshWLdPu61boHUzOW+Q4yPHOTRwiNMTp0m0NJIIf4DJ9ASJXIKsKgATMPmKPlwIELICupEPtBDxR+ho7CDii9Q0/hF/hKgvSsRfDW+wG+Z8CThw4MC8FJURCMPCkM9A34+1MFx6Tb95feCTsPv3oGPHrXmGZesGP9YFPFAnD2mIX5zdA5k4r/OUmaxN749CUxcE9A56CsU0inFVJE6RCUpVV7RbE0aJSUqU5nixDyohhocYHpqUh9V4aMJDTIVoLofjIaY8RPEwlLXxd+9kKtJJMtxKytdIqpgmlU+RyqVI5pKV81Q+xeD0IMl4Nayorr3de4PdUBGXmb2UsC9MxBdhc/NmdrTuIOq/dfuPzwvFPIyfc4TAMQsNvwljb0ExV00X7dICsGG/IwJbYMUmCEQALfJnJs5w8MpBDr75HY4MHyFdSOMRD+uj62kKNLE2so5I647aRt4bIpKdIjp5mcjIWSLD/YQGTmKVeyP+KHTugM4OWPEBWLVTm0WXUE/QCESxABd+Df4w+CP6CESWxMDT+4KJ83D4O3Dke5Aeh5ZuePjPYMenIbjAW5d6g9C6SR/1SMch/i7DQ2/QN3yMvvjb9KcHuaKuEKfEBEXygn4NnIGtIIZVadQ34nMaeYsm5bjlRl9sYngIisu8NbNRmOlXJcIjpwn+6mA1zPLphqx1C7RtgdYd0LZV94Jm9ASUUmSKGS0k+SRTuSmS+WRFTJK5JFP5qVkiM5md5HLqciU863rD3hjbyI7WHfS29dLT1kNXuGtxzFyFHIyfrTULjZzWZstSvpoutkaXVfdDld4AKzaDPzTrlheTFzl04SUODhzk1YFXmchOALA+up5PbPwE93Tcw66Vu4j4Iu8tr8WCzueVo3DlCFw+Aq/8z2o+G1Y4Yxm9WjA6d0K4/UZL5qYxApGJw1O/OTvc8rlEIwyBqON3hfnDWkzKwjIrLLw8B0lLRW3See1b8NZLeixgyz+B3V+EdR9aMm9ISimGpofoG+ujf7yfvrE++sb6GE2PAiAI66Lr6Ip0cWegmZg/RpO/iVig6jb7m4kFYoS8oXlvHA8dOMD++3a73oYdE8nFV+Gka2NGOwAruvXgadsWaN2KtG0hGFtLsCFIK603nIfp/DQnR09ydPgox0aO8dL5l3jurecAaA4009PaQ09bD71tvWxr2YbP8t3sn60HiadHITkAyUFIDrL23D/C0Ld1WYyfdY09iRbI1i2w+WGXEGy66oy38cw4rw68ysGBgxwcOMjl1GUA2oJtPLD6AfZ27GXPyj20N95kY23ZsPJOfez8rA4rZGHopBaLK8e0cJz9mZ6sAXrMY9VO6OzRgtHZCw3NN5eP68QIhD8Mj/9fyCb1kUlANuH4E7Xh8YtOWEL7r6PLjh10iUa4tqdSEx6hdfgKDLfrt2zrNvzXTI3B0e/pHkP8XT0I/KGvwc7HIbq4u32VxeDU2KmKEPSN9TGe0Vudl80F93Xex7aWbWxr2cbmps00eBsWNd+z8DXqxmLVztrwbFLPqhnprwrHu7+GN75fTWOXe1BV4aBtizaxXOdgfYO3gT0de9jTsQfQ5pez8bMcGznGsWF9/PzizwHwerxsb9lOb1svO9p20NPaQ0uwpXqzSsM/6BwDkBqqEQKSgzpsxm9tDR5oXqcFYOujtULgvfaOiNP5aY4MH9Fmo4GDnJ44DUDYG2b3yt18btvn2Nu5l3WRdfPfK7L9sOpufZTJTcHACS0WV45q8Xjzb6vxTWurYrFqJ1ZhetZtbwUyX/NnF5pdu3apw4cPL9wDlYJCxhGUZFU4aoSmfng6myCeTzGRTxIvZoh7hAnLQ9LjwacUQSwC4ZUEo10EmtYTaOkmsGILwcZWgnaQgB0gYAcIWkFsj724s1eUgkuHdW/h1I/0IN/aB/TYwpZHb6oHdeDAAfa7ZzFdd5YUg1OD9I31aUEY76N/rH+WGJSFYHvLdjY1bVp6YjCDGyqPTMLpcfRr+3vZTV6ppvE2auFo2+aYq7ZqN7r6hnp7o1MjHL/0MscGX+Xo2Cn6UhfJOw18Fz56ikJvOk1PYpT12czsr3WDzRDugPBKx22v+kMrIbySXx55kw99+KPXnad8Kc+p0VO8MvAKhwYOcXzkOIVSAa/HS29bL3s79rK3Yy9bW7Zie5boy1k6DgPHnJ7GUX1MXgQg1biG0L87cUO3FZHXlVJ1Zn2YHsSNIwLeIFmPhwmPIu4pEPfkiEuGCRHiKCYkT5wsE0wzSYoJ4sRVnIydcW4Sdo56TMP0m/q4PHc2LLG0YFiBinhURMQKVP2ueHeY2+9OVx6Q9M7VwOemtVnj1b+CwRPgC+tpf7u+oBuYBUIpxcDUQE2voG+sr2IztsRiQ2wD+1bvY2vzVt0zaN5M0F4m+24HInDHbn24ScdrzVTD/fD238Oxp6tpfGH9Vu7ubTRvgFwKkq43/ZSrB5AcYkVqkI+UCnzEuU1WoM/n42i4iWNBi5dtxfMhD4TaCHv87AivobdlO70de9i++n4aAtcem1Kes1ePV4qz8bMVk9HhocNM5acQhK0tW/nsts+yt2MvvW29t09dCMZg/X59lEmNwJWjnDtxlLvm4ZFGIFzkijk9/zyj553PnIo4kXGmJDrTEieyE6Td86NnEPaFK7bqtoY2NjVtoinQRNQfnWXLbvI3ceSVI9x7/71kihkyBX2kE5fJjPSRGTtDevwd0pMXyKSGyAhkREjbfjKhAJlggLQVJuNtIG17yZTyjGfG9X2KGdKFNOlCmkwhg+L6e40BK0DYF64eWISTg0TGzhHOZwg3tBK57wuE1+0n3NhKxFMknHhXp/WG5xaYG0ApxeXU5Roh6B/vJ56NA2CLzYbYBvbfsb/SO9jUtImAfZWps8uVYAy69urDzfT4bOE482Jljn/9ezXpN/xQux70Da90HR34Q+30htrpdaYwK6W4kLygxzEcs9Q/vvMjeOdHWGKxpXkLPW3OWEZr73Xb/QenBnnlyiscGjzEoYFDlbGkrnAXj6x7hL2de9ndvpvYdQjQbUOoFTZ9jLErt2Cspw7LXiBG06N85oXPEM/GmcpPzZku5A3pQcpAEy2BFjbGNlb8lQbf8cf8MaL+6Hvuqlpi0eBtqDV1xNZD14wpm/m0/uEOnYTBN2DwJJw7qU1ZAAi0bICVd0H7Lu2uvAvCHSggV8pp8XEEY6aATBemK9Mmk7kkiWycxNhbJEfOMJGNc8FjkWxsJClBCioHA3+vjzoE7SBhb7hWZJwj4otcNWw0P8qL51+sEYPJrJ6KaovNxqaNfLjrw2xrdsSgeRN+y8w+uykammHNffpwMzWmzVPj7+jxs4rJp/3q367UQURYE1nDmsgaHtv4GACT2UmOjxzXgjFyjOfOPMfT/bo309HYoQWjVQ9+dzd1V655bfA1Dg4c5NDAIc4nzgN6sLxsMrqn4x46Q503VybLmGUvECFviN623prG3X3eFGgi6ove0jfhm8YbnD1QqRTEL2jBKAvH5SN6XKBMsBlZeSf+9rvwr7yL6Mo79RufPcfbR3IIjjwFh/9G26wjq2HXl2Dn5yDUVlkaoCwkybwjKLlENczlT+QSjGXGOJ84X4m75tz8K2B7bLpj3TzU9VClZ9Dd1G3EYCFpbIHG+2Ht/fNy+6g/yr7V+9i3eh+gxwxOj5/m2PAxjg4f5fXB1/m7c38H6G81ohJl8NlBFIoGu4FdK3fxyc2f5J6Oe+iOdZuvym8RZpB6AVFKkcgUGEtlGU3lHFefj6ayvPXuZdau7iAS8BIJeokGvUSCdsWvXZto0EvQa13fjyCTgKFTjmic0L2N4T49wA7g8Wo788q7oN2ZfofoBfP6n9fTBzd8RE9R7f7YLZ1dVRaYuQTl3NvneOy+x+iOdd+a6ZK3OTc6aH+rUEpRUmB5Fr7xLY81lc1Sx989zoObH2Rv517uXHEnXs8SeoFbBG6mbphB6nmkUCwxMZ13GvosY05jX270a8RgKkeuUJp1DxFobvDhVSUunBklkckznbv6m7XtEUc07KqYBGYKio6LBDcQad9CdO2ndZxfCCTO1/Y2zv5Cr4tUJhCDe76sB51bNtziUiv/3VIxqa1snL2/8IGhA2xv2T4vz16KKKXIFkpk8yUyhSKZfJF0vkgmXyKTL3JytEjpzSFyBUWhVCJfLJEvKHJFfV4oVs/1oarnBee8pMgXdJj7Ou2vpq+5V8G5V6mEUrruNfgsGnw2DT6LoM+6ql+H2TR4nXC/k87rSue3aPBa2Fb96bYiQmeok85QJ4+sf4QDmQPs79m/sP+gJUq+WCJdMKu5LhiZfLHSyM98y3eHjaVyjE/n6m5Z4LM8tIR8rAj5WRHysXlluHK+IuR3xflpavBiW56at4B8sUQyU2AynSeRzpPI5EmkCyQy+TnDrsTTJDIFEuk82TpCVJM/20MkECMS3E8k8FEizV5WtU/Rrc4Rs9JcankAy9eA75Tgs8/jtz34nMNvW/rcKvs9lfhKnBPvteS26+4rpSiWFIWSoqQUuUKp0kjrBttptAtFsq4GPJMvkimUSOeKTpwr3NXoZ+YIzxZK197+4jp7yZZH8FqC1/I4hz73WR5sV7jP8uD3emj029pv6zjbUz33Otf4LA+WR8g6f+NUtsB0vkg6V2Q6VyA+neNyvOqfzhWvWQ9n4rM8LoHR4uH2B702jX6L0cEcbxTfotFvE/JrgSmfN/oc1wnz254lXQeVUmTypcrveDKdZ3La+Z27w9LO7z1dG5bOF9kY8/AbD936vC17gZiYyvHED0+4Gv4cqWz9lRHDfrvSsK9b0cjutc20hPy0Vhp9LQAtIT+RwM19n+C1PDQ3+mhuvDHTSiZfrBEQLSj1BSeRzjM5neNiRngpfQfJTIFc8eIN592NCDOEpCoufm9VZHwVkbEqcYMDWV4cf4OSq7EulBTFUoliSVWOSlzRCVOqJt6druIvi0CxRElBoVSiVHLcW/AyVv4bgl6LgNci4PVo17YIB2xaw37HrxvE8rnfnd6uvbbvjePs3nU3XqexdjfcXsuD1/Zge3SjvhhmoHoUS4rpXMERDX2k8wWmstXz6VyxEj/lSlsWmqlckbFUjovluHyRVDrPC+fOXFcebI/Q6Ldp9GnBqAiJW1TqxtuEnDSNPieN38Jvz17QUClFKluoacjLv7VK2KzGPs+k0+DnilcX0pDfdkzOXqJBm7UrGogEtOUgGvQyNfTuDf1/rll283LX2wif7eH86DQtIR8fWB2rvN23ut7yy27Ae/usdBlwGpq2uT6zuAZKaRNDrlAiW9BurqDNEtl8iVyxWAl3x+vz4qxrszXX6jTu61LZgitOp89kCwTGh7A9guUctkfwlF0RbMtxnXif7alN66SxPB4sQbueqmt7quktj2CJ1Po9ugGuaeQrruU04i6/V4vgfDTQ+UsWPXfcXlM0LY8QDngJB27tGMGBAwe47/59TOcKpLJacLSrj8q509PRYc65c81IMqvT5XR8vnh9bwZeSyqiYVtSaeyv9mLhEWpMwdGgl85o0DEB25WGPjojTTToJRyw5zS9VcvjKh9L3QTzKhAi8jDwDcACvqWU+vqM+C7gKSDmpHlCKfWCiKwF+oHTTtKDSqkvz0ceG/02L35133zc+rZGRPDb+m3pBjXmplnsQVnD0kb3PH3EGm7NBIZsociUS0TcolIrPsWKMBVLqtKYuxv6GjFo8BLy2XiWSK/uvTBvAiEiFvA/gI8Cl4DXROR5pVSfK9mfAN9XSn1TRLYBLwBrnbizSqme+cqfwWAwuCm/EN2oWff9yHxup7UHeFsp9Y5SKgc8C3x8RhoFlNfLjQJXMBgMBsOSYN6+gxCR3wYeVkp90fF/FrhHKfUVV5oO4CWgCWgEHlJKve6YmE4BZ4AE8CdKqZfrPONLwJcA2tvb73722Wfn5W9ZKFKpFKHQ7LXplyumPGox5VHFlEUtN1MeDz744JL9DuKfAd9VSv1nEbkX+J6I3AkMAF1KqTERuRv4sYhsV0ol3BcrpZ4EngT9odztbq82NvdaTHnUYsqjiimLWuarPObTxHQZuMPlX83sdUl/D/g+gFLqFSAArFBKZZVSY07468BZYI5twAwGg8EwH8ynQLwGdIvIOhHxAZ8Gnp+R5gLoVYFFZCtaIEZEpNUZ5EZE1gPdwDvzmFeDwWAwzGDeTExKqYKIfAV4ET2F9TtKqVMi8qfAYaXU88AfAX8lIl9FD1j/c6WUEpF9wJ+KSB4oAV9WSo3PV14NBoPBMJt5HYNQSr2AnrrqDvsPrvM+4IN1rnsOeG4+82YwGAyGqzOfJiaDwWAw3Ma8b5b7FpERYH4WJFk4VgCji52JJYQpj1pMeVQxZVHLzZTHGqVUa72I941AvB8QkcNzzUdejpjyqMWURxVTFrXMV3kYE5PBYDAY6mIEwmAwGAx1MQKxtHhysTOwxDDlUYspjyqmLGqZl/IwYxAGg8FgqIvpQRgMBoOhLkYgDAaDwVAXIxCLhIjcISK/EJE+ETklIn/ohDeLyP8Tkbcct2mx87pQiIglIkdF5G8d/zoROSQib4vIXztrei0LRCQmIj8QkTdFpF9E7l3mdeOrzu/kpIg8IyKB5VQ/ROQ7IjIsIiddYXXrg2j+m1MuJ0Rk540+1wjE4lEA/kgptQ3YC/wrZ1e9J4CfKaW6gZ85/uXCH6K3mi3zZ8B/UUptBCbQq/8uF74B/FQptQXYgS6XZVk3RGQV8AfALqXUnei13T7N8qof3wUenhE2V334DfQCp93o/XK+eaMPNQKxSCilBpRSR5zzJLoBWIXede8pJ9lTwGOLk8OFRURWA48A33L8AnwY+IGTZDmVRRTYB3wbQCmVU0rFWaZ1w8EGgiJiAw3oPWOWTf1QSv0DMHPB0rnqw8eB/600B4GYsznbe8YIxBLA2UGvFzgEtCulBpyoQaB9kbK10PxX4Gvo1XsBWoC4Uqrg+C+hBXQ5sA4YAf6XY3L7log0skzrhlLqMvAX6O0BBoBJ4HWWb/0oM1d9WAVcdKW74bIxArHIiEgIvXLtv66zY55CL4P+vkZEHgWGnc2hDPpteSfwTaVULzDFDHPScqkbAI5t/eNo4exEb08809yyrJmv+mAEYhERES9aHJ5WSv3QCR4qdwcdd3ix8reAfBD4pyJyHngWbTr4BrprXF6Svt6OhO9XLgGXlFKHHP8P0IKxHOsGwEPAOaXUiFIqD/wQXWeWa/0oM1d9uJ7dPK8LIxCLhGNj/zbQr5T6S1fU88DjzvnjwE8WOm8LjVLq3yulViul1qIHH3+ulPpd4BfAbzvJlkVZACilBoGLIrLZCfoI0McyrBsOF4C9ItLg/G7K5bEs64eLuerD88DnnNlMe4FJlynqPWG+pF4kROR+4GXgDap29z9Gj0N8H+hCL1/+yeW0m56I7Af+rVLqUWe72WeBZuAo8BmlVHYx87dQiEgPesDeh95u9/PoF7plWTdE5D8Bn0LP/jsKfBFtV18W9UNEngH2o5f1HgL+I/Bj6tQHR0T/O9oMNw18Xil1+IaeawTCYDAYDPUwJiaDwWAw1MUIhMFgMBjqYgTCYDAYDHUxAmEwGAyGuhiBMBgMBkNdjEAYDA4i8mvHXSsiv3OL7/3H9Z5lMCxlzDRXg2EG7m8x3sM1tmtdoHrxKaVU6Fbkz2BYKEwPwmBwEJGUc/p14AEROebsQ2CJyJ+LyGvO+vr/wkm/X0ReFpHn0V/2IiI/FpHXnb0LvuSEfR29EukxEXna/Szna9c/d/Y5eENEPuW69wHXnhBPOx9AISJfF72PyAkR+YuFLCPD8sK+dhKDYdnxBK4ehNPQTyqldouIH/iViLzkpN0J3KmUOuf4v+B8zRoEXhOR55RST4jIV5RSPXWe9VtAD3rPhxXONf/gxPUC24ErwK+AD4pIP/AJYItSSolI7Jb/9QaDg+lBGAzX5mPotW2OoZdCaUFvxgLwqkscAP5ARI4DB9ELpnVzde4HnlFKFZVSQ8Avgd2ue19SSpWAY8Ba9FLXGeDbIvJb6KUUDIZ5wQiEwXBtBPh9pVSPc6xTSpV7EFOVRHrs4iHgXqXUDvT6QIGbeK57XaEiUB7n2INe4fVR4Kc3cX+D4aoYgTAYZpMEwi7/i8C/dJZnR0Q2ORv4zCQKTCilpkVkC3or2TL58vUzeBn4lDPO0YreSe7VuTLm7B8SVUq9AHwVbZoyGOYFMwZhMMzmBFB0TEXfRe9NsRY44gwUj1B/e8ufAl92xglOo81MZZ4ETojIEWcp8zI/Au4FjqM3fPmaUmrQEZh6hIGfiEgA3bP5Nzf2JxoM18ZMczUYDAZDXYyJyWAwGAx1MQJhMBgMhroYgTAYDAZDXYxAGAwGg6EuRiAMBoPBUBcjEAaDwWCoixEIg8FgMNTl/wPPCoUFSHadkQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219d32d2-76dc-4448-d68e-ebc386e3b95f"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "# we wiil iterate on some mu and batch size values.\n",
        "# we will run the simulation on a fixed value for 'max_iterations' = 150\n",
        "max_iter = 150\n",
        "wb_dict = {}\n",
        "\n",
        "# for mu in range(10):\n",
        "#   for batch_size in [100, 150, 200, 250, 300]:\n",
        "#     w,b,cost_vec, acc = run_gradient_descent(w0, b0, mu=0.05+mu/100, batch_size=batch_size)\n",
        "#     print(f'mu={0.05+mu/100}, batch_size={batch_size}, we get accurecy of {acc}')\n",
        "#     wb_dict[f'{cost_vec[-1]}'] = f'mu={0.05+mu/100}, batch_size={batch_size}', b, w \n",
        "b,w,cost_vec,acc = run_gradient_descent(w0,b0,mu=0.1,batch_size=150)\n",
        "\n",
        "print(f'for accurcy {acc}, we get\\nb = {b}\\nw={w}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for accurcy 0.7251333333333333, we get\n",
            "b = 0.31763543254170323\n",
            "w=[ 1.33763731e+00 -9.21970159e-01 -2.03236541e-01 -2.81780872e-02\n",
            " -9.90695029e-02 -6.99131109e-01 -2.25732831e-02 -2.12567416e-01\n",
            " -1.97409364e-01 -8.29815315e-03 -2.61519483e-01  7.16837381e-02\n",
            "  2.16383501e-01  1.68612699e-01 -9.16619689e-02  1.28332453e-01\n",
            "  2.33893131e-02  2.86471981e-01  1.20901741e-01  1.29615757e-01\n",
            "  2.74312183e-03  3.27257558e-02  3.17355060e-01  8.39330328e-02\n",
            " -1.44028166e-01  1.57988331e-02  1.24884586e-01  2.44115731e-02\n",
            " -3.01867656e-03  6.93753053e-03  1.64591929e-02 -2.30881636e-02\n",
            " -1.33033907e-01  9.86739133e-02 -2.61440359e-04 -1.30538877e-01\n",
            " -2.62695808e-02  7.79812614e-02  1.13260097e-01 -4.70962940e-02\n",
            " -8.63268232e-02 -2.57626650e-02  2.21097590e-02 -2.66003867e-02\n",
            "  6.56668619e-03  8.03141924e-02  3.51936723e-02 -1.08723930e-01\n",
            "  5.97634904e-02 -1.50951379e-02 -1.85539519e-02 -8.02869366e-03\n",
            "  2.03277015e-02 -2.00010885e-02 -5.06403649e-02 -1.87941604e-02\n",
            " -1.54927619e-01  6.16851879e-02 -4.39270550e-02 -4.45518815e-02\n",
            " -7.15323721e-02 -1.45711031e-02 -1.32869903e-01  8.99860161e-02\n",
            " -1.35918339e-01  1.36044547e-02  8.37003788e-03  7.22124773e-02\n",
            " -8.74776827e-02 -2.52563555e-02 -7.53745922e-02  1.63766564e-02\n",
            "  1.65743869e-02  6.58599482e-02  4.82758801e-02  6.48337082e-02\n",
            "  3.97135321e-03 -8.27934705e-02 -1.15124304e-01 -1.09301835e-03\n",
            " -1.82915874e-02  1.35006697e-02  1.87531379e-02  1.03083589e-02\n",
            "  5.75836889e-02 -2.47973567e-02  5.87686910e-02 -2.13712521e-01\n",
            " -3.99247372e-02  2.09498236e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI",
        "outputId": "f174a47a-772a-4b98-df92-126c500873f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "train_acc = get_accuracy(pred(w,b,train_xs), train_ts)\n",
        "val_acc = get_accuracy(pred(w,b, val_xs ), val_ts)\n",
        "test_acc = get_accuracy(pred(w,b, test_xs ), test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.561135080913189  val_acc =  0.5595  test_acc =  0.5613984117760992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ",
        "outputId": "f8209944-6597-4b04-f7fd-34dd1e86aca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.linear_model as lm\n",
        "\n",
        "model = lm.LogisticRegression()\n",
        "model.fit(X=train_norm_xs, y=train_ts)\n",
        "\n",
        "pred_train = model.predict(train_xs)\n",
        "pred_val = model.predict(val_xs)\n",
        "pred_test = model.predict(test_xs)\n",
        "\n",
        "train_acc = get_accuracy(pred_train, train_ts)\n",
        "val_acc = get_accuracy(pred_val, val_ts)\n",
        "test_acc = get_accuracy(pred_test, test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.5611326637902904  val_acc =  0.55956  test_acc =  0.5614371489444122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    }
  ]
}