{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aradgast/DeepLearningCourse/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "load_from_drive = False\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/DataBase/YearPredictionMSD.txt'#.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "aebca2c7-7f5d-43a9-c989-7186e0510993"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
              "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
              "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
              "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
              "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...        ...   \n",
              "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
              "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
              "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
              "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
              "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
              "\n",
              "            var84     var85     var86      var87     var88      var89  \\\n",
              "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
              "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
              "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
              "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
              "...           ...       ...       ...        ...       ...        ...   \n",
              "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
              "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
              "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
              "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
              "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "515340 -15.46052  \n",
              "515341  10.88815  \n",
              "515342  -8.09364  \n",
              "515343  39.74909  \n",
              "515344  12.17352  \n",
              "\n",
              "[515345 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a69f060-9026-41b1-82e5-acb0f80b0b92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>...</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>...</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>...</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows × 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a69f060-9026-41b1-82e5-acb0f80b0b92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a69f060-9026-41b1-82e5-acb0f80b0b92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a69f060-9026-41b1-82e5-acb0f80b0b92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "72c8ef39-181f-43b6-d930-24b35f68f0d8"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ac6b6f6-7e13-4457-a76a-a1f6fb0ed602');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here -> our explanation:\n",
        "# Parts of the var are reffering to the artist's data, for instance : the artist's id, artist's  location, and others...\n",
        "# those featchures might be studied by the neural network and can help it understand the correct outcome\n",
        "# in those cases, of course that the neural network will succeed in recognition of the test set\n",
        "# but it's only imply that the neural networks working good at data that is similar to the training set and\n",
        "# the result of the test will be faulty.\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "# when we calculate expected value by mean, we want to use as many samples as we can, the more samples by 'large numbers law' the better the estimation is.   \n",
        "# the same for the estimation of variance.\n",
        "# because the test set is with the same nature if the train set, we assume that the variance and expactancy are the same.\n",
        "# for this reason we will use the mean and std we computted with the train set, also for the test set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here ->\n",
        "# we should use the validation set to avoid overfitting - not letting the model\n",
        "# to 'memorized' our data set and by so preventing a flase low loss cause\n",
        "# by overfitting.\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  e = 0.0001\n",
        "  return -t * np.log(y+e) - (1 - t) * np.log(1 - y + e)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  return sigmoid(np.dot(X,np.transpose(w)) + b)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred(np.zeros(90), 1, np.ones([2, 90]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD0655W16X02",
        "outputId": "bc2ae9f3-abab-46c7-b0ca-8c177b34ae1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.73105858])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "        \n",
        "  \"\"\"\n",
        "  err = y-t\n",
        "  dLdb = np.mean(err)\n",
        "  dLdw = np.dot(np.transpose(X),err)/len(err)\n",
        "  return dLdb , dLdw\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "The cost function is: \n",
        "\n",
        "$$L = \\frac{1}{N}∑_{(i=1)}^{N} -t_{i}log{y_{i}}-(1-t_{i})log{{(1-y_{i})}}$$\n",
        "\n",
        "by using partial dertitve we can get,\n",
        "\n",
        "$$ \\frac{∂L}{∂b} =  \\frac{∂L}{∂y}\\frac{∂y}{∂z}\\frac{∂z}{∂b}\\quad,y=σ(z), z = w^Tx+b     $$\n",
        "where each part can be evaluted elmentwise,\n",
        "\n",
        "$$ \\frac{∂L}{∂y_{i}} =  \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} $$\n",
        "$$ \\frac{∂y_i}{∂z} =  σ(z)(1-σ(z)) = y(1-y) $$\n",
        "$$ \\frac{∂z}{∂b} =  1 $$\n",
        "\n",
        "in total we get,\n",
        "$$ \\frac{∂L}{∂b} = (\\frac{∂L}{∂y})^T\\frac{∂y}{∂z} = \\frac{1}{N}∑-t_i(1-y_i)+y_i(1-t_i) =\\frac{1}{N}∑y_i-t_i   $$\n",
        "if we sign,$$ error = y - t$$ will get,\n",
        "$$ \\frac{∂L}{∂b} = mean(error) $$\n",
        "\n",
        "in the same way we could evalute the derivative in respect to w,\n",
        "\n",
        "$$ \\frac{∂L}{∂w} =  \\frac{∂L}{∂y}\\frac{∂y}{∂z}\\frac{∂z}{∂w}\\quad\n",
        ",y=σ(z), z = w^Tx+b     $$\n",
        "$$ \\frac{∂L}{∂y_{i}} =  \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} $$\n",
        "$$ \\frac{∂y}{∂z} =  σ(z)(1-σ(z)) $$\n",
        "$$ \\frac{∂z}{∂w_i} =  x_i $$\n",
        "\n",
        "in total we get,\n",
        "$$ \\frac{∂L}{∂w} = \\frac{1}{N}error^T * X $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "outputId": "003a2ea9-cf32-476f-bbf2-df7db7d077c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code goes here\n",
        "h = 0.000001\n",
        "w = np.zeros(90)\n",
        "b = 1\n",
        "x = np.ones([2, 90])\n",
        "t = np.ones([2,])\n",
        "y = pred(w,b,x)\n",
        "y_h = pred(w, b+h, x)\n",
        "derivative = derivative_cost(x, y, t)\n",
        "\n",
        "\n",
        "\n",
        "r1 = (cost(y_h,t)-cost(y,t))/h\n",
        "r2 = derivative[0]\n",
        "\n",
        "\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - -0.268904540134951\n",
            "The algorithm results is -  -0.2689414213699951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "outputId": "f6f4c859-5935-4b35-c1c8-33f8ccb52453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "\n",
        "r1 = np.zeros([90,])\n",
        "for i in range(len(w)):\n",
        "  h_vec = np.zeros(w.shape)\n",
        "  h_vec[i] = h\n",
        "  y_h = pred(w+h_vec,b,x)\n",
        "  r1[i] = ((cost(y_h,t)-cost(y,t))/h)\n",
        "\n",
        "\n",
        "r2 = derivative[1]\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - [-0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454\n",
            " -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454 -0.26890454]\n",
            "The algorithm results is -  [-0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # declare as a global vars\n",
        "  global train_norm_xs\n",
        "  global train_ts\n",
        "  global val_norm_xs\n",
        "  global val_ts\n",
        "\n",
        "  train_x = train_norm_xs\n",
        "  train_label = train_ts\n",
        "  valid_x = val_norm_xs\n",
        "  valid_label = val_ts\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  vec_val_cost = []\n",
        "  vec_tr_cost = []\n",
        "\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_x))\n",
        "    train_x = train_x[reindex]\n",
        "    train_label = train_label[reindex]\n",
        "\n",
        "    \n",
        "\n",
        "    for i in range(0, len(train_x), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_x[i:(i + batch_size)]\n",
        "      t = train_label[i:(i + batch_size), 0]\n",
        "\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      y = pred(w, b, X)\n",
        "      \n",
        "      # update w and b\n",
        "      dLdb , dLdw = derivative_cost(X, y,t)\n",
        "      b -= mu*dLdb\n",
        "      w -= mu*dLdw\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "      if (iter % 10 == 0):\n",
        "\n",
        "        # tr_y = pred(w, b, train_x[:50000])\n",
        "        # tr_cost1 = cost(tr_y[:20000], valid_label[:20000])\n",
        "        # tr_cost2 = cost(tr_y[20000:40000], valid_label[20000:40000])\n",
        "        # tr_cost3 = cost(tr_y[40000:], valid_label[40000:50000])\n",
        "        # tr_cost = (1/5)*(2*tr_cost1+2*tr_cost2 + tr_cost3)\n",
        "        # vec_tr_cost.append(tr_cost)\n",
        "        # tr_acc = get_accuracy(tr_y,train_label)\n",
        "        # print(\"Iter %d. [Train Acc %.0f%%, Loss %f]\" % (\n",
        "        #         iter, tr_acc * 100, tr_cost))\n",
        "\n",
        "        val_y = pred(w, b, valid_x)\n",
        "        val_cost1 = cost(val_y[:20000], valid_label[:20000])\n",
        "        val_cost2 = cost(val_y[20000:40000], valid_label[20000:40000])\n",
        "        val_cost3 = cost(val_y[40000:], valid_label[40000:])\n",
        "        val_cost = (1/5)*(2*val_cost1+2*val_cost2 + val_cost3)\n",
        "        vec_val_cost.append(val_cost)\n",
        "        val_acc = get_accuracy(val_y,valid_label)\n",
        "        # print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "        #         iter, val_acc * 100, val_cost))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "      #need to be added for plotting\n",
        "      \n",
        "  return b,w, vec_val_cost\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "outputId": "ca6090e3-7577-4b8d-f1df-c95cefa5f53c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "b,w,cost_result_001 = run_gradient_descent(w0, b0, mu=0.01,batch_size=200, max_iters=150)\n",
        "print('Done for mu=0.01!')\n",
        "b,w,cost_result_005 = run_gradient_descent(w0, b0, mu=0.05,batch_size=200, max_iters=150)\n",
        "print('Done for mu=0.05!')\n",
        "b,w,cost_result_01 = run_gradient_descent(w0, b0, mu=0.1,batch_size=200, max_iters=150) \n",
        "print('Done for mu=0.1!')\n",
        "b,w,cost_result_05 = run_gradient_descent(w0, b0, mu=0.5,batch_size=200, max_iters=150) \n",
        "print('Done for mu=0.5!')\n",
        "b,w,cost_result_12 = run_gradient_descent(w0, b0, mu = 1.2,batch_size=200, max_iters=150)\n",
        "print('Done for mu=1.2!')\n",
        "b,w,cost_result_5 = run_gradient_descent(w0, b0, mu = 5,batch_size=200, max_iters=150)\n",
        "print('Done for mu=5!')\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done for mu=0.01!\n",
            "Done for mu=0.05!\n",
            "Done for mu=0.1!\n",
            "Done for mu=0.5!\n",
            "Done for mu=1.2!\n",
            "Done for mu=5!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "iter_vec = [(i+1)*10 for i in range(len(cost_result_01))]\n",
        "plt.plot(iter_vec, cost_result_001, label='mu = 0.01')\n",
        "plt.plot(iter_vec, cost_result_005, label='mu = 0.05')\n",
        "plt.plot(iter_vec,cost_result_01, label='mu = 0.1')\n",
        "plt.plot(iter_vec,cost_result_05, label='mu = 0.5')\n",
        "plt.plot(iter_vec,cost_result_12, label='mu = 1.2')\n",
        "plt.plot(iter_vec, cost_result_5, label='mu = 5')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cost value')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mIAFIZEWoQp_",
        "outputId": "45afaa77-7fab-4e7a-cfd9-efa15b59a954"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3wcxfn/33NFujvdnXovlmW5yh0ZDAYjYwhgEnqHEEgC38CPkBBaCr04oSYkhARIDDExJXRjDBgXmeJuY4O75are20mncnfz+2NPJ8mWpZOt00nWvF+vee3e7Ozusyr7uWeemWeElBKFQqFQDF10wTZAoVAoFMFFCYFCoVAMcZQQKBQKxRBHCYFCoVAMcZQQKBQKxRDHEGwDektMTIxMT08PthmdaGhoICwsLNhm+M1gsncw2QqDy97BZCsMLnsHoq0bN26skFLGdnVs0AlBeno6GzZsCLYZncjNzSUnJyfYZvjNYLJ3MNkKg8vewWQrDC57B6KtQoiDRzumuoYUCoViiKOEQKFQKIY4SggUCoViiDPoYgQKhWJw09raSkFBAU1NTb06Lzw8nB07dgTIqr4lmLaaTCZSUlIwGo1+n6OEQKFQ9CsFBQXYbDbS09MRQvh9Xn19PTabLYCW9R3BslVKSWVlJQUFBQwfPtzv81TXkEKh6FeampqIjo7ulQgo/EMIQXR0dK+9LSUECoWi31EiEDiO5WerhEChUCj6kNamJtwtLcE2o1coIVAoFIo+wtXSQlVxIc211Xjc7iOOSym54447yMzMZOLEiWzatKnL62zcuJEJEyaQmZnJHXfcQdu6Me+88w5ZWVnodLo+nVirhEChUCj6AI/HQ01pMUJoL/yGmuoj2nz66afs2bOHPXv28PLLL3Prrbd2ea1bb72VV155xdf2s88+A2D8+PG8//77zJw5s09tV0KgUAxgnI56Dn63OdhmnFAcOHCAMWPGcOONNzJq1Ciuu+46li5dyowZMxg5ciTr1q0D4OGHH+aZZ57xnTd+/HgOHDjQ5TWllNRXlOFqaSE8LhGDyUxjbQ1ul6tTu48++ogbbrgBIQTTp0+npqaG4uLiTm2Ki4upq6tj+vTpCCG44YYb+PDDDwEYO3Yso0eP7sOfhoYaPqpQDFBqy0p4b+6DVBcXcc1jz5A0akywTepzHvl4G9uL6vxq63a70ev1PbYbl2TnoR9lddsmLy+Pd955h3nz5jFt2jTeeOMNvv76axYuXMjcuXN9L15/uOqqq9ixYzselwud3oBOr8ftdnPLT27gJz/5CfbYOF/bwsJCUlNTfZ9TUlIoLCwkMTGxU5uUlJQj2gQSJQQKxQCkdF8e7//pYTwuF0aTmS1LPjkhhSBYDB8+nAkTJgCQlZXF7NmzEUIwYcKEo37rPxr/nf8fqgoLCDGbiUhIQghBfX09ssmJs74OS0Qkhl5M7goGSggUigHGgS2bWPjcHzFZrVz54B/ZvGQR3y/7nDNv+DkWe3iwzetTevrm3pG+nKQVGhrq29fpdL7POp0Ol7c7x2Aw4PF4fO26Gpvvcbu54vLLydu7D0OIEdCGbno8Hn5z5685/8yZNFRXER4XD0BycjL5+fm+8wsKCkhOTu50zeTkZAoKCrpt09eoGIFCMYDY/uVyPnjyESLi4rn2sWeITkll0jlzcLtcbF3xRbDNG1Kkp6f7RvVs2rSJ/fv3dzoupaS2vJSXnv8LmzZtZPPmLWzevJnNmzfzzTffcONNP8ViD8dZX4fLO5z0wgsvZP78+UgpWbNmDeHh4Z26hQASExOx2+2sWbMGKSXz58/noosuCuizKiFQKAYAUkrWffQun/79OZLHZHHVI09ijYoGICZ1GCnjxvPd0k/xeI4ckqgIDJdddhlVVVVkZWXxwgsvMGrUqE7HG2uraW5owBodQ4jJ3OU1LJGRCJ0OR3UlAHPmzCEjI4PMzExuvvlmXnzxRV/byZMn+/ZffPFFfv7zn5OZmcmIESM4//zzAfjggw9ISUlh9erVXHDBBZx77rl98qyqa0ihCDIej5sVr73C5s8XMWbGmZx766+P6FOe/IMLWPSXJzmwZRMZU6YFydITg/T0dLZu3er7/Nprr3V5zGw2s2TJki6v0eJspL6yEpPV2m13nV5vICw8Akd1Fa0RTRhDTfz973/vsu3mze2jw7KzszvZ2MYll1zCJZdc0u3zHQvKI1AogoirpYVFf3mSzZ8v4qQfXsKc2+/qMrCYOW06YRGRbFmyOAhWKjridrmoKS3BYAzBHhPXY0oHS3gEOr0OR1VVP1nYe5QQKBRBosnh4N0nHmDP2lXk3PBzcn78M4Su639JvcHIhNnnse/bDdSWlfSzpYo2pJTUlpUgPR7C4xPQ+TGcVafXYwmPpLmxgZYmZz9Y2XuUECgUQaCuopy3HrqXkrxdXPCreznpgot7PGfi7HMRQrBl6Wf9YKGiKxxVlbQ4ndhj4zB2GHnUE5pXoMdRVelLFzGQUEKgUPQz5YcO8OYDd1NfWcGlv3uUMaf5ly7AFh1DZvZ0ti5f4huFoug/mhocNNRUY7GHY7bZe3WuTqcjLDKKFqeTFufA8wqUECgU/Uj+tu94+6H7QEqufuRJ0sZP7NX5k34wB2d9HbvXfhMgCxVd4Wptpa68FGNoKLbomGO6hsVmR28w4KgeeF5BwIRACGESQqwTQmwRQmwTQjzSRZtQIcTbQog8IcRaIUR6oOxRKILNrtVf8d7cBwmLjOKax58hdpj/K0i1kTZ+EpFJKWxe8kkALFR0hfR4qC0tBgnh8YlHjeP0hPB6Ba1NTTQ3NvSxlcdHID2CZuAsKeUkYDJwnhBi+mFtfgZUSykzgT8DTwbQHoUiaGxa/BGLnn+K+BGjuPrRp7DHxPV8UhcIIZh8zvkU795J6f69fWyloivqKytobW7GHhd/3KkiTFYbDzz+BGOzxh9TGuqHH36Y5ORkJk+ezOTJk1m8uG9GkQVMCKSGw/vR6C2H+0MXAf/x7r8LzBZq6SLFCYT0eFj533ms+M8rjJx2Kpff/xhm6/GlSRh35mwMIaFsUV5BwHHW19FYV0tYRCSmMOtxX++zzz7jYGEhq5Yu4W9/+XOv01AD3Hnnnb4ZzHPmzDlumyDAE8qEEHpgI5AJ/F1KufawJslAPoCU0iWEqAWigYrDrnMLcAtAfHw8ubm5gTS71zgcjgFnU3cMJnsHk63Q2V6P283BFZ9RtWcHseMnY5synW9Wre6T+0SMGMXWL5ejSx+FIdR03Lb2J+Hh4dTX1/f6PLfbfUznHc7Bgwe59NJLmTZtGmvXrmXq1Klcf/31zJ07l/Lycv71r3+RnZ3NE48/TqhOcPuttyKNIYwbN47//e9/DBs27Jhtfffdd7n66mvQG42MzRhOVVUVe/bsISEhwdempKSEmpoasrKycDgcXHHFFbzzzjucfvrpNDc3YzQae/w5NDU19ep3G1AhkFK6gclCiAjgAyHEeCnlkdPler7Oy8DLANnZ2TInJ6dvDT1OcnNzGWg2dcdgsncw2Qrt9jY3NrLw2Seo2rOD06/5CSdfdHmfrtNbmpbCf3/3a6LczUzNOe+4bO1vduzY0Z487tPfQsn3fp3ncrsw6P14ZSVMgPP/dNTDVquVffv28d5775GVlcW0adP48MMPWb16NQsXLuT555/n/fffw9PSjDCbiUpKRm8woNPpsFqtRyS+u+qqq9i1a1enOo/Hw913380NN9zQqb6srIxRo0Zhj4mlpqSY5KQkamtrGTlypK/Nrl27SEtL891n5MiRvP/++9hsNkJDQ3nllVd4++23yc7O5tlnnyUyMvKIZzSZTEyZMqXnn5WXfkkxIaWsEUKsAM4DOgpBIZAKFAghDEA4UNkfNikUgcJRXcX7f3yIyoJDnHfbnWSdObvP7xGfkUniyNFsXrKYKedfqBaD7yU9paGuKy/D43ZjstnQG7p/Tb799ttH1PWUKTXUEobRZMLjakV2yHDaE7feeisPPPAAQggeeOAB7rrrLubNm+f3+UcjYEIghIgFWr0iYAbO4chg8ELgJ8Bq4HJguRxo46oUil7QVF3Jmw/cjbOujkvufZD0yScF7F6Tf3ABn/79OQ5t3cKwCZN7PmEg0s0398Nx9lMa6paWZpocDiw2G7oOHkhXaaihdx5BWxpqIQTWyGiKikuICrcf0eZoaajj4+N99TfffDM//OEPe/PYRyWQo4YSgRVCiO+A9cAXUspFQohHhRAXetv8G4gWQuQBvwF+G0B7FIqAUrhrBzs/eBNXSwtXPfyngIoAwKjpp2Oy2VX+oT6kpbkZj8tNqCWMkWPGdpuGuo23337bF7ztmIb6cBGAzmmoN23Zgj3cji00pNO6B92loe64rOUHH3zA+PHj++S5A+YRSCm/A47opJJSPthhvwm4IlA2KBT9RfGeXbz7+P0YzBaueexpIuITej7pODGEhDBh1jlsWPQB9ZUVxzzRSaHhcbupLy9DCEF4XDyXX345r7/+OllZWZxyyilHpKE+FubMmcPixYvJzMzEYrHw8j//gcftprG2htNnneXLQPriiy9y44034nQ6Of/8831pqO+99142b96MEIL09HReeuml47YJVBpqRR+wf/NGcuf/i9k/vbXXM2VPBOorK/jomccJi4xk2HmX9IsItDHpnPNZ//H7fLfsc2ZceV2/3Xcw01UaaiklNSVFpCQl8t2WLej0+m7TUB8rQogj0lBXFxfRWFvNpo0bfXVHS0P9+uuv96k9bagUE4rjorWpiS9eeYGqwnzem/sA3y/v23+cgU5rcxMfPv0Yrc1NXHzPAxgtYf16//C4BDKmZPP9ss9we5dYVPSehppqmhsbscXEYjQd23DcY8UaFY3H7aGxtqZf79sRJQSK42LNB29TX1HOJfc9RGrWRJa89FdW/nder0ZCDFaklHz2j+cpO7CPC+64l5jUnseXB4JJP5hDQ001eevXBOX+g53mxkYcVZWYrbZeJ5PrC4yhoZisVhpqa3C7gyPmSggUx0xVUQEbPv6AcTPPImPqNC797cNMOmcOGz5+n4XPzaX1KKMsThTWvP8Wu1d/xcxrbyRjavBWDUufNJXwuHg10/gYcLtaqS0rwRASgi2250VmAoU1Mhrp8dBYXR2U+yshUBwTUkqWv/oSxtBQZl53E6AtwDH7Z7cy68Zb2LthHW89dB/1VRU9XGlwsnvtN6z63wLGzTyL7B9dGlRbdDo9E88+n/zt31ORfzCotgwmpJTUlJYgpSQiPhHdMSaT6wsMISGYbXYa62pxu1r7/f5KCBTHxO4133Dwu2+ZcdX1hEW0z2wUQjD1/Au5+N4HqC4p4o3f/4bSfXlBtLTvKTuwj0///hyJI0dzzs23D4jJXONnnYPeaGTLF2ooaXe4XS6cdXXUlBZTfnA/rU1N2GPjMISEBNs0wiKjAGgIgleghEDRa1qanOTOf4XY9AwmndN10quMqdO45rGnEXo9bz18H3vW902OnWDTUFPNh089hslq46K77x8QLxAAiz2c0dNPZ/uXy2lxNgbbnAGD9HhobmykvrKCivxDlB/cT215KS1NTkItYUQkJB53EsC+wmA0YrbZcdbX4WrtX69ACYGi16x+900cVZWc/bNbu12zNTYtneueeI7Y1HQWPjuX9QvfG3ALcvQGV2srHz37BE5HHRff80AnT2ggMOkHF9DidLLj69xgmxI0pJS4WlpoqK2huriIsoP7qC4upLG2Bp1ehy06muiUNGLThhMeF98nGUV7a98dd9xBZmZml2mo27yC395zN6mpqVit/WOfEgJFr6gsOMSmxR8xftY5JI0a22P7sIhIrnhoLqOmn86XC15lyUt/DUof6PEipWTpK3+nePdOzr/tTuKHjwi2SUeQOHI0cekj2Lxk8aAW3N7icbtpanBQV15GRf5BKvIPUl9Rjqu1BbPNTkRCErHpGUQlpRAWEYUxNDRo3XmffvqpL7X0yy+/fEQaar3BgCU8glmnn86qr7/qN7uUECj8RkrJsnn/JMRk5oxrb/T7PGNIKD+84x6mX3oVW1d8wXtPPIjTcfzphPuTjYs+YNvKpZx6+bWMmn56sM3pEiEEk34wh4pDByjctT3Y5gQMKSWtzU04qquoKiyg/OB+akqKcTrqMYSEYI+JIyYtndi0dOwxcZjCwjoFgg8cOMCYMWO48cYbGTVqFNdddx1Lly5lxowZjBw5knXr1gHaIjDPPPOM77zx48dz4MCB47L9o48+4oYbbkAIwfTp06mpqemUNgLAEhFB9klTsXXIhxRo1MziAUh1cSE7v/mShBEjGT4lO9jm+Ni56kvyt33H2T+/DYs9vFfnCp2OGVf9mMikFJb883nevP8uLrnvISITkwNkbd+x79v1rFzwKqNOmcGpl10dbHO6ZeyMM/nyv/PYsmQxKWOygm1Ojzy57kl2Vu3ssZ2UUpswJyXS4/F5PEKnQ6fTodPrfUtIjokaw30n39ft9fLy8njnnXeYN28e06ZN44033uDrr79m4cKFzJ07lw8//NDvZ+hN0rnCwkJSU1N9n1NSUigsLCQxMdFXp9cbCAuPwFFd5bcNx4sSggFCc2MDu1Z/xbaVyynyfpvTG41c+eBcv7pgAk1zYyMrX/838RkjmTD73GO+zrgzZmGPjWPhM0/wxh/u4sK7fk9q1sBNS1FZcIhPnn+auGEZnHfbnce8Xm1/YTSZyDpzNpuXLCbnhp8PuDjGseB2ubTuRKl5PUKvQ6/TXvzH2sXTUxrq3nAsaah7whIeQWNdLfRTF9+QEYLCndtZ9uo/yZgyjeFTskkcOQqd7uiBzv7A43FzaOt3bMtdSt661bhaW4hKTuWMa29k+JRsFj7zBB898wTXPv4s4XHxPV8wgKx+dwENNdVcfPf9x/1zSxmTxbVPPMcHTz7Cu088yDk3/z/GzzqnjyztO5z1dXz41GMYQkK46J77+z31wLEy6Qdz2PTpQrau+IJTLrky2OZ0S3ff3D0eD/UVZTjr6wkNC0MXYsIeGdkn/fvdpaF2eVN1GAyGTllB+zINdRsdU0x3RKfXExYRgQRanE5CzObePWAvGTJC4PG4CTGZWffRO6z94G1MVhvpk6aSMXUa6ZOm9uvU8qqiAratXMb2L5fjqKokNCyMrJyzycqZTcKIUb4/9Ivve5A377+bD596lKsffZpQi6XfbOxI+aEDbPr0YyaedS4JmcefgREgIj6Bax57mkV/eZLP//k8VUUFnHHNTwbMN263y8Wiv/yJ+spyrnzoj9hjYoNtkt9EJaWQNn4SW774lGkXXRb0LzzHgru1lZrSYlqbm7FGRREWEYXD4ejXIG96ejqLFi0Cek5DfThH8wguvPBCXnjhBa6++mrWrl1LeHh4p26hjpjtEQA4qiqJTEoO6LMPGSFIHTeBqx95kiaHgwPfbWL/txvYv3kjO79ZiRA6EkaO8nkLcekZff5Db2pwsGvVV2z7chnFu3cihI70yVPJueFmRpx0cpfj0aOTU/nRnb/jvT8+yCfPP8nF9z7Y7XDNQCClZNm//0FomJXTrzkyv/rxYAqzcsl9D7HitZdYv/A9qouLmHP7XQPim/eK/7zCoa3fcd5tdw6IrrneMvncC1j47Fz2bdpAZvYpwTanV7Q4G9tn/CYkYQrr30R+bVx22WXMnz8/oGmoX331Vd+xyZMn+9JQ33vvvbzxxhs4nU7Gn5TNT3/2Ux5/Yu5x3/9oDBkhaMNktTLmtJmMOW0m0uOhZN8e9n+7gX2bNvDN26/zzduvExYZxfDJ2WRMyWbYxMmEmI/tm7jH4+bQd5vZunIZeetX425tJToljZnX/5Sxp+dg9Y4Z7o5hEydz9s9u44tXXiD39X9x1o3/d0y2HCs7vlpB4c5tnHPLLwPiNekNBmb/7DaiklLInf9v3n7kt1x8zwNYo6L7/F7+snnJYrYs+YTsH10akGUm+4MRJ52CNSqaLUs+GTRCIKWksa6W+spyDMYQIuITAzJhr6s01F0d66801G20iQDAU089xVNPPYWUHiryD6HT6ZFSBswrGHJC0BGh05GYOZrEzNGcdsV1NNRUs3/zRvZ/u4Hda75m64ol6PQGUsaOY/jkbIZPnUZUUkqPv4zKgny2fbmMHV8ux1FdhclqY8JZ55J15mziMzJ7/cucePZ5VBXls/GTj4hKTGHyuRccz2P7TVODg5X/nUdC5igmBLAPXwjB1DkXEZGQxKLnn2LBH37Dxfc+GJSx+oe2bmH5q/8kY+o0zrj2J/1+/75Cp9cz8ezzWPW/BVSXFBGZkBRsk7qlYzzAFBaGPTa+373fgYgQOqwRUdSWl9Lc2BCwCXBDRgg259fw6jf7iTAbCbeEEGE2EmHRSrg5hHCzkQiLhdFnnMX4nLNxu1wU7d7h9RbWs/K/81j533mEx8UzfMo0MqZkk5I1AWOIFmRqcjjYuepLtq1cSkneboROx/Ap2cw6czYZU0/GYDQel/0zr/8p1cVFLH/tJSLiEwK+DCLAqncW0FhXy6W/fbhf+u4zpk7jmkef4oMnH+Wth+7lgl/eE/B7dqS6pIiPn/sjUUkpzPnlPYOyb70jE846lzXvvcUW7wiigUqneEBkFGGRUQMif9NAwWSz0VBbrcUTLWEB+dkMGSGoamhmc34NNY2t1DW1djsqyxpq8AqDkQjLWCKmTiJisoPwyjwcRbvYsuxzNn++CJ0xhJiRWTQ0Ovj2lb/gcbmISUvnzB//jLGn5/Tp0D2dTs8Fd9zDWw/ey8d/eZJrHns6oPnvyw7sY/NnnzDpnDnEZ2QG7D6HEztsONfNfY4Pn36Mj559goTJJ1M9emTA5xs0Nzbw4VOPgRBcfM8DQQvM9yXWyCgyTz6NbblLmXHV9RhDgx97OZyBEg8YyGgL3UdRU1pCk6M+IF20Q0YIzhoTz1ljtCGYbo+kvqmVmsZWapyt1DS2UOtspdbprWtspcbZQq33+M7aOu+xBFy6ePTJp5HcVES68xDpu/di9LSw2zqWA5HjkFFJRO4PJbJsD1EWI5FhIURZQogMCyHSEkJkmJEob12EJYQQg//ftEPMFi6+70EW/P43fPDko1w397leT+zyB+nxsOzf/8BktXL6VT/u8+v3RFhEJFc+9EeW/POv7PxmJfO+XUt0ShqZ004lc9r0Y+pe6w6Px80nf32ampIiLvv9Y0QkdD2KYzAy+Qdz2L36K3at+mpADdHtr3jAiUJomJUQszlgqUOGjBB0RK8TRHhfxL1BSklji7tdPLxCsfbbrUxPHc6ohhaqG1qoatS2ByoaqG5oob756KsO2UINRIQZfWLRLhpGr41GzTvxdl+Fh0Vy0d0P8M6jv+Ojpx/nigee6PN/oG0rl1G0ewfn/uJXmPop6dXhGENCueCOezAOH02MQbB3w2rf0F9rdAyZ2dPJnDadlLHj0RuO78/4qzf+w/5vN3D2z2874dZcThk7nuiUNDYvWTxghEBKSV15qW9+QLiKB/SIEILIxMANIR2SQnCsCCEICzUQFmogOaJ9goelchc5OUfvPmlxeahp1ASiqqGFmsZWqg4TjWpvXV6Zg+qGFhpa3Ee9nk7AhPizmLn7M35/1wOUTL6UCEuIrzsr3OwVj05CYsRuNmIydv8P1+Rw8OWCV0kaNXZAjJgJsdmZmpPD1PN/hLO+jn2b1pO3fg1bV3zB5s8XYQqzkjF1GpnTTiV90tReDz3dtnIZGz5+n8nnXnDUlNqDmbb8Q8vn/ZOSvN19Ng/kWKmrKKOhphpnqFHFA3qJmkcwyAkx6Iizm4iz+/+Samp1U+fUPI72LquOXVjDqNncSvyOZTh3RfNd7DRqnK3UOVvxdOM9mow6zDpJ/OYvifQKRZtgRFqMyG/ew+moZ+z517K3ooFws3bMqA/+RC+zzU7WmbPJOnM2rc1NHPxuM3nrV7N34zq2f7UCgzGEYZOmkJk9nYyTTu6x26xw1w6+ePlvpI2fSM4NN/fTU/Q/4844i68WvMbmJYs5L4hCkL/9ez7+85846Yb/IyIhsd9TQA8EpJT86le/YvHixVgsFl577TWmTp16RLucnByKi4sxe2cUL1myhLi4uIDZpYRggGIy6jEZ9d2Kh7woi0//7oGvVnD7Jacy+tRZeDyS+mYXtY1ewXC2+ISk1tultXNfPqZwCzWNLewpc/hEJtJZxlVFX/KdfQJ/W1gEFPnuZQ01eMWiXTwiLUbv6KsQ37HwDnV2kwFDgATEGGoic5rWPeRxuyncuY0961eTt34NezesRQgdyWPHkZmtxRUOT9FRV1HGwmefwBYdyw/v/N1xdy8NZEItFsbNnMW23GWc+eOf9vsC7VJKvv3sY3Ln/4vIhCSskZFDUgSgcxrqtWvXcuutt7J27dou2y5YsIDs7P5JOnni/vUPAYQQ/OD/7qC2rJTP/v5n7DFxJI4c7esaOhq5uWXk5HT+A/O43fz3/rupt4fz63t/yc+l0RcLqWlspdq7rWnUurHyqxp93kp38SubSROQCHOHbqojPnvFxWwk3FsXavC/z1in15OaNZHUrInM+sktlO3fS96GNeStX0Pu/Fd8q6mN9AabI+IT+fDpx3G1tHDlg38cMCtUBZJJ58xhyxefsi13ab+usdza0szSV/7O9i+XMyL7FM7/f3ex72Bw11U+cOAA5513HtOnT2fVqlVMmzaNm266iYceeoiysjIWLFjAySefzMMPP4zVauXuu+8GtDTUixYtIj09/ZjvfbQ01EdLM9FfKCEY5BiMRi66+w+88Yff8OHTj3Hd3Oewx/Tehdyau5TyfXs4///9hnEj/Z981DYCq9orEjXOVi2I7t3v6InUOFsprHb6BMTdTR+WJURPuNmI3t1M0s7V2EwGbCYDdrPRu2/EbjJ2qrObDNhNRuwpwzlt+AhmXHk91SVF7F2/hj3r17Dq3TdY9c4CQsxmWpuaueS+B4lOST2qDScSscOGkzxmHFu++JSTLri4X+aF1FWUs/DZJyjdl8dpV1zH9EuvOuK+JXPn0ryj5zTUAC63myo/gsqhY8eQ8Pvfd9tmIKehbuOmm25Cr9dz2WWXcf/996sYgaJ7LPZwLrnvId64/24+fPJRrn70qaxF0KwAACAASURBVF6lxXDW1/HVm/8heUwWY8+Y1at7dx6B5f8YcCkljmZXB6HQurE6CUdjK3n5xeh0UFLXxO6yVuqbXNQ3uboVEQCDTnQQiDhsqZcSkdZETFUeYaV7MA4fR25jNJs2FmgelEUTljZvymQ89hTHA5VJP7iAxX99moPffRvwCYlt8QB3awsX3fPAgEtzMdDTUC9YsIDk5GTq6+u57LLLeP31148Qlb5ECcEJQnRKGj+687e8/6eH+eSvT3PRPf6ni/76zfk0NziY/bNb++3lJ4TAZjJiMxnp7jt5bm41OTmndqprG8ariYI2QbDOKxB1Tk0s6ppaqW9q7VS3t0nHt+7h1FtTcRS4oODoq3gZ9YJw70irjgJhN2uTDTvXte83tEo8HolON/BEZNQpp5EbHsHmLxYHTAg6xgMiEpK46O4/EJ189N9wT9/cO3K8L9eODPQ01G11NpuNa6+9lnXr1ikhUPhH+qSpnHXTL1j27xf58r/z/BoJU5y3i++Wf85Jcy4kNi098Eb2AR2H8SaEH9tsWZfbQ32Ti1qnJiRto7HqnK72/aa2Os1DOVjZQJ33nO48Et3yxT5xiOggFIfPCbEfMdzXiNmoD5gY6w1GJpx1Lus+fIe68rI+uaaUkvrKCiq9awUX7NjKvk3rvfGA3xBqGbwzhYOVhtrlclFTU0NMTAytra0sWrSIs88+uw+e6OgoITjBmPyDOb4EdZGJKUw65/yjtvV43Cz79z8Ii4jk1Muv60crg49Br9Mm7oX1fjKelJKGFm14b7uAaNtNW3cSk5TWaaZ6rbOVgmqnr647EdE8kRDCzQZfIF3zQAydPBS71ztp80zsZiO2UEOPnsjEszUh+G7ZZ5CQ1qvnbqyt8S0O31Yq8w/R4mz0tQmLjGLGlddzyiVXDpi1JY6VYKWhbm5u5txzz6W1tRW3283ZZ5/NzTcHdnizEoITkJwf/5ya4iKWzfsHEfGJDJs4uct23y/7nNJ9ecy5454TIrdOfyGEwBpqwBpqICmi88pRsY695OSMPuq5bbGRNpFoE5Caw4Sjzjv0t6y+id2l9b7uru5GaAmhzVTv2J11uFjYTQbMmRPY+PmnmC+4kZSyes0zMbenO2lqcFCZf6jDy17bOuvrfPcy2ezEpKYxbuYsYlKHEZ2SRnTqsEExAmugp6EOCwtj48aNfXrfnlBCcAKi0+u54Ff38daD9/Dxn//INY8/c0Q/bWNdLV+/OZ/UrImMOW1mkCwdenSMjaT0MiehxyNxtLh84lHndHXqvqrzxkPqOnRrHaho9O03emerpzamcnHjFj5c8R3/+qaIqJYqoluqiHNVE9VahaXV0X5PQwhEJmBMGYc9PoXwpDRi09KIiY3xDfu1hhpOuMD6UEMJwQlKqMXCJfc9xII//IYPnnyEax9/ttNM26/eeI2WJiezf/oL9U88SNDphNYtdAwiAtDqjYvUNDTz2SPrmV2xsv2g3ognPJammEzKLDFUhURTqo+g1G2i2umipckDB4GDTli9C2gPjup1whcPaRt91T7Et+OQXwO2UCMxbg/OFjd6nfZMeiHU32CQUUJwAmOPjeOiu+/nf4/+joXPzuXy+x/HYDTiKCli14ovyP7RpUSn9K6fWDF4Mep1WubbsBAu+uVdfPnJR2SfcSYxqcMIj0/odpRZU6u7U6qTtq6s2sauZ6/nVzVqHkpTKy0uT6drvXJhIrqy+k51eiE0UfAKg17n/Szw1btaJJ7GFq1Nh6JTQnLcBEwIhBCpwHwgHpDAy1LK5w9rkwN8BLSF49+XUj4aKJuGIkmjxnDerb/mk78+zdJXXuAH/3cHh75aijUqmlMvvybY5imCRPKYcSSWlDHy5NP8at+W8iS+F/my2mh2uX3zP+qcrXiqCxgWZcEttQmJbqkNuXV7JB6pbVvdHtwuicejtZFowZGKDoHpNgTiCHHQHyYqhqMc1ykBAQLrEbiAu6SUm4QQNmCjEOILKeXhg7e/klL+MIB2DHnGzDiTqqJCVr/7BjWlxTgryvjhr39LiMnc88kKxXESatATatUTY9XG6u9wFBPeixTwUkqkhNr6ekyWME08OhWPb9/lkbg8Hppd7ce7QyfaBaFdHOgkJLojvBRxwnVrBUwIpJTFQLF3v14IsQNIBo4+i0cRME69/BqqiwvZ+c1KbCnDGDV9RrBNUij8QgiBENpscXMPadQPR3o9DLc8XDw6lMOOtXTwTjx+LARzeLeWTifwuD3UuZ0+ATmap6ITgU0v7S8iUCvedLqJEOnAl8B4KWVdh/oc4D2gAC3V5d1Sym1dnH8LcAtAfHz8SW+99VbAbe4NDocDa5AWcOkNHpeL0i0bMKWkExmfEGxz/GKw/GzbGEz2BsvW8PBwMjN7v/yp2+1G388L2Egp8Ui0Au37viI7f25r45G+/Y7sz9vNg3fdzo6tW/jlPfdz4y9+iU5onom21cpdt93M91u+xWA0MmXqSTz93F8whRgx6DRB7Im8vDxqa2s71c2aNWujlLLLdKYBFwIhhBVYCTwhpXz/sGN2wCOldAgh5gDPSylHdne97OxsuWHDhsAZfAzk5uaSk5MTbDP8ZjDZO5hshcFlb7Bs3bFjB2PHju31eX2ZYiLQtNl6uEdSUlLKgYMHWbTwI+wREfzf7b/u0kNZ8cXnnJajzSb+7e0/56RTTuPKG35GrC2UxPCeu3S7+hkLIY4qBAGd+ieEMKJ9419wuAgASCnrpJQO7/5iwCiEiAmkTQqFYmhz4MABxowZw4033sioUaO47rrrWLp0KTNmzGDkyJGsW7cOgIcffphnnnnGd9748eN7nZBOCIFBryPUoMcSYiAjLZmzzjgNe5gZa6iRxHAzKZEWhkWHkRFrZWS8jTEJdm798RVMTIkgKzmcs844jZa6CjJiwojs5fK6/hLIUUMC+DewQ0r53FHaJAClUkophDgZTZgqA2WTQqEYWHz1v91U5Dt6boj/XUMxqVbOuLL7dBDBSkPdG4QQSJeLN99YwPPPP4/VdPQ1Ro6XQI4amgH8GPheCLHZW/d7IA1ASvlP4HLgViGEC3ACV8v+CFooFIohzUBPQ93GbbfdxsyZMznjjDOO+1rdEchRQ18D3UY1pJQvAC8EygaFQjGw6embe0dOhDTUveGRRx6hvLycl1566Ziv4S9DZ2axW/vloh86j6xQKI6dQKSh9pd//etffP755yxbtgxdP2RxHdx5YnvD/lx4KgPevh42zIPqA8G2SKFQDGAuu+wyqqqqyMrK4oUXXuiTNNQlJSWkpKTw3HPP8fjjj5OSkkJdnTaifs6cORQVFQHwi1/8gtLSUk499VQmT57Mo48GNuHC0Pl6HBYHWRdB3nLY8bFWF5UBI87SSvoZYLIH10aFQhFwgpmGOiEhgYKCgi6PLV682Lff1j3VXwwdIUicCBf+DaSEyjzYu1wrm9+E9f8CoYfUk9uFIWkK+LnUo0KhUAxmho4QtCEExIzUyin/B64WKFjXLgwr5sKKJ8AUARlntgtDhMrSqVAoTkyGnhAcjiEE0k/XyuwHoaFSiyfsXa51I23/SGsXndmhG+l0CB0cMxwVCoWiJ5QQHE5YNIy/TCtSQsXudm/h2//CupdBZ4DUU9qFQbqDbbVCoVAcM0oIukMIiB2tlem3gqsZ8te2C8Pyx2D5Y8wwWKH4dEg7FYadBomTNU9DoVAoBgFKCHqDIRSGz9TK2Q+Doxz25VK++m2SKvfC7s+87cyQkg1p0zVxSD1ZdSUpFIoBS49C4M0ZdB2QIaV8VAiRBiRIKdcF3LqBjjUWJl7B7qpYknJyNGE4tForB1fBV8+C9GgjkhImaN5C2qlascYG23qFQtHP5ObmctFFFzF8+HAALr30Uh588MEgW+WfR/AiWprts4BHgXq0jKLTAmjX4MQaC+Mu1ApAcz3kr/MKw2ptItuaF7Vj0SNh2KmQdpq2jRimdUUpFIoTmjPOOMM3Y3mg4I8QnCKlnCqE+BZASlkthFAd4P4QaoPM2VoBLcZQtBkOrdKEYftHsGm+dsyW5BUGb5whdiz0w9RyhWKoceDAAc477zymT5/OqlWrmDZtGjfddBMPPfQQZWVlLFiwgJNPPpmHH34Yq9XK3XffDWhpqBctWkR6enpwHyAA+CMErUIIPdoC9AghYtE8BEVvMYRC2ilaOf1O8HigbHt7V9LBVbD1Pa2tKaJdFIbN0CbE6QOXhlahCAYrXnuZsoP7/GrrdrnRG3qe5Bk3LINZN97SbZtgpqFevXo1kyZNIikpiWeeeYasrCy/7xUo/BGCvwIfAHFCiCfQUkffH1Crhgo6HSSM18rJN2vDVasPwKE1cPAbTSB2f6q1NYZpQedhMzRxSD4JjKagmq9QDFaClYZ66tSpHDx4EKvVyuLFi7n44ovZs2fPMT1DX9KjEEgpFwghNgKz0dJKXyyl3BFwy4YiQkDUcK1Mvkarqy/1diV5y4onAAn6EEjO9noMp6mRSYpBSU/f3DtyIqShttvb85nNmTOH2267jYqKCmJigrswoz+jhtKARuDjjnVSykOBNEzhxRYPWZdoBaCxSpvLcPAbTRi+/jN89Yw2MilxUntXUtp0sEQF13aFYhATiDTUJSUlxMfHI4Rg3bp1eDweoqOj+9bwY8CfrqFP0OIDAjABw4FdQPA7toYiligYfb5WAJodWq6kNo9h3Suw2rvWT1xWu8cw7DSwJQTPboVikHHZZZcxf/58srKyOOWUU/okDfW7777LP/7xDwwGA2azmbfeegsxAEYL+tM1NKHjZyHEVOC2gFmk6B2h1vZUFwCtTVC0qV0YtrwJ61/RjkWNgGGnktgQDqVx2oxplWFVMcQIZhrq22+/ndtvv71Pr9kX9HpmsZRykxDilEAYo+gDjKZ2DwC0ldlKvmsXhp2fMNpZDbv/DiFWLd12SrYWb0jJVl6DQjEE8SdG8JsOH3XAVKAoYBYp+ha9AZKnauW020FK1n76Bqck66FgAxRugFV/A493IQx7iiYIbeKQOAlCLMF9BoVCEVD88Qg6RjxcaDGD9wJjjiLgCIHTkgyTcmDS1Vpda5PmNRSsbxeH7d5x1EIP8VkdvIZpWkpuNdlNcRxIKQdE3/iJiJSy1+f4EyN45JisUQwejCZt+Gnqye11jjIo3KgJQ8F6+P5dLUUGQGi45mF07FIKC+7wN8XgwWQyUVlZSXR0tBKDPkZKSWVlJSZT7+YYHVUIhBAf451NfJQbXtirOykGF9a4zqOTPB5tbYbCDe1eQ1tSPdByJSVP1VJwJ03RupTMEcGzXzFgSUlJoaCggPLy8l6d19TU1OsXXLAIpq0mk4mUlJRendOdR/DM8ZmjOKHQ6SBujFamXK/VtTRouZMKN2jeQ+FG2PZB+zlRIzRRSPKKQ8JEMNm7vr5iyGA0Gn3ZN3tDbm4uU6ZMCYBFfc9gshW6EQIp5cr+NEQxCAkJg/QZWmmjsQqKvm0v+Wth67veg971otu8hqQpWnruUGtQzFcoFBr+jBoaCfwRGIc2oQwAKWVGAO1SDFYsUZ0zroK2TkPx5nZxOPA1fP8/7ZjQQcyodmFImgLx49VIJYWiH/Fn1NCrwEPAn4FZwE1ow0gVCv+wxsLIc7TSRn2J1q1U9K0mEnnLtMlvoI1Uih0DSVNIarDAwVBt5JLqVlIoAoI/QmCWUi4TQggp5UHgYW8SuuAvq6MYvNgSYPR5WgEt82p9sddr8ArE7s8Y1VgBe17W2kQO92Zrnah5DQkTIDxFLeijUBwn/ghBsxBCB+wRQtwOFAKqU1fRtwgB9iStjLlAq5OS1Z+/x6kZNm2eQ8n3ULIVdnzcfp4pQhOEthI/XvMmDGrtJIXCX/wRgl8BFuAO4DG07qGfBNIohQIAIWg2xcCoHBh1bnt9cz2UbofS79vFYcOr4HJqx3VGTQwSxreLQ8IElY1VoTgK/giBW0rpABxo8QGFIriE2tpXemvD44bKvR3E4XvYu6I97gBa+gyfOGRp2VmjMrQ0HArFEMaf/4BnhRAJwLvA21LKrT2doFD0Ozo9xI7SyvjL2usd5Z3FoWQr7PkCpFs7rg/VsrDGZ0HcOIgfpwmELUHFHhRDBn9STMzyCsGVwEtCCDuaIDwecOsUiuPFGgvWDmm6AVqdUL5LWy+6dJu2Pdx7MEdqghA/rt17iBujVoFTnJD45RNLKUuAvwohVgD3oo0Y6lYIhBCpwHwgHi1VxctSyucPayOA54E5aKug3Sil3NTbh1AoeoXR7J3tPLlzfWNVuzC0bb9dAK0N7W0ihh3pPURnqu4lxaDGnwllY4GrgMuASuBt4C4/ru0C7vKuX2ADNgohvpBSbu/Q5nxgpLecAvzDu1Uo+h9LFAw/QytteDxQc9ArDtuhbJu23f15h+6lEIgZDfHjSHWEwq4miBsL4akqS6tiUODP15h5wFvAuVJKv9chkFIWA8Xe/XohxA4gGegoBBcB86WWN3WNECJCCJHoPVehCD46HUQN10rbsFbQUndX7O7sPez/ihH1RbBvvtbGGKZ1J8WO1YShbd+epOIPigGFOJbc1b2+iRDpwJfAeCllXYf6RcCfpJRfez8vA+6TUm447PxbgFsA4uPjT3rrrbcCbnNvcDgcWK2DZ2rFYLJ3MNkK0FRTSpyoIqzhEGENBwlryCes4SAhrbW+Ni59GA1hqTSEDeuwTaPVGN6vAjHYfraDyd6BaOusWbM2SimzuzoW8I5NIYQVbSGbX3cUgd4gpXwZeBkgOztb5uTk9J2BfUBubi4DzabuGEz2DiZbQbN3as5VRx5oqICyHVC+E0PZdsLLdhJethaKP29vY4n2eg9jNA+izZMI0PyHwfizHSz2DiZbIcBCIIQwoonAAinl+100KQRSO3xO8dYpFCcWYTFHxh+kBEepJhBlO6Dcu93yNrTUdzg3TgtIR2doqb2jR2ifI4er5HyKPsGfYPEVUsp3eqrr4jwB/BvYIaV87ijNFgK3CyHeQgsS16r4gGLIIIQ2X8GWACNmtddLCbUFUL7TKxC7oGov7F4CDWWdr2FP1ibFRY/wikSmth+ZDobQfn0cxeDFH4/gd8DhL/2u6g5nBvBj4HshxGZv3e+BNAAp5T+BxWhDR/PQho+qmcsKhRAQkaqVjhlbAZrqNFGo3AtV+7zbvbB9ITirOlxDpyXk6+hBtO1HpIHe2L/PpBjQdLdU5floL+lkIcRfOxyyow0N7RZvALjbyJd3tND/889UhUKByd6+bsPhOKuhch9U5nUQi73w3TvQ3B6sRmeAiDQmEAXNX2gzq2NGa7OyzZH99yyKAUN3HkERsAG4ENjYob4euDOQRikUimPAHAkpJ2mlI1JCY2W7MFTuhco8Qg9uhnWvgLu5vW1YnFcYRmkldpQmEmrI6wlNd0tVbgG2CCHekFK2AgghIoFUKWV1fxmoUCiOEyG0YHVYTKdEfRtyc8mZeYY2Ya5ijxaLqNgF5bu15UWbOngRITZtmdFOIjFaC1irWdWDHn9+g18IIS70tt0IlAkhVkkplVegUAx2dHot2ByV0TnVt5TgKPMKwy5t8lz5Lti3snNOJp1Rizu0CUPMKG88IgPMEf3/PIpjwh8hCJdS1gkhfo42C/ghIcR3gTZMoVAEESHAFq+V4TM7H2uq0zyIjiJRug12LgLpaW9nifaKzIh2sYn2blUsYkDhjxAYhBCJaNlH/xBgexQKxUDHZO86FuFqbo9DVO1rH9V04Gv47rBsAOaoDuIwooNgDFcLCAUBf4TgUeBz4Bsp5XohRAawJ7BmKRSKQYch1Ju2e9yRx1qdUH2gw5BXr1AcWg3fv4OWoNiLKeIwcdAEw9hSq3VZqaB1n+PPegTv0GHOgJRyH1omUoVCofAPo9mbeG/skcdam7SAdUeBqNoL+Wvh+3dpE4kZABts2mS5yGHebboWsI4armV7VWtVHxP+zCxOAf6G9/cAfAX8SkpZEEjDFArFEMFo0gLNsaOPPOZqhuqDULWXPeuXMjLaoHkWlXmQtxRcTe1thU6bae0TCG+JGq6JhTlSeRNHwZ+uoVeBN4ArvJ+v99adc9QzFAqFoi8whPqWIC0sNjOyYyI3j0fL1VR9wFv2t+/v/vzIdByh9g6exHBtGzEM7IlgSxzSQuGPEMRKKV/t8Pk1IcSvA2WQQqFQ+IVOp73E7Ykw7NQjj7c0aN5ER4GoPgBlOzWhcLd0bq8P8eZ+Sux+G2o/4QTDHyGoFEJcD7QNHr4GbaUyhUKhGLiEhB09eO3xQH0x1BzStvUlnbel2yFveecssG0YLZ2FwZpwhGDoOs7WHgT4IwQ/RYsR/BktarMKlRxOoVAMZnQ6CE/WSnc0O7Tup67Eor4Eir6FumJwOTudNhNgU6yW4C8iTeuC6rRN1QLoAwR/Rg0dRMs3pFAoFEOLUKtWokccvY2U0FzXLhB1xezb8jUZkXptNFTxFtixCDytnc+zxnchFGla7CI8pV/TiPszaug/aKOEaryfI4FnpZQ/DbRxCoVCMeARAkzhWvGOfDpUk0jGEYHtEi1mUXPIWw5qpXAjbP8IPIcldbYlHikSKdkQn9Xnj+BP19DENhEAkFJWCyG6yIGrUCgUii7R6bQMrvakrgPbHjfUFXUQiTahOAT5a2DreyDdcPqdQRMCnRAisi3jqBAiys/zFAqFQuEPOn37YkS+KVsdcLugrjBg3UX+vNCfBVYLIdpmF18BPBEQawLIjsod/Gf7fxgfPZ7xMeMZEzUGk8EUbLMUCoWiZ/QGbQ5EgPAnWDxfCLEBOMtbdamUcnvALAoQpY2lrC9ezyf7PgHAIAyMjBxJVkwWE2ImkBWdxYiIERh0ytlRKBRDC7/eet4X/6B7+XckJzWHnNQcShtK2Vq5lW0V29hasZXPD3zOu7vfBcBsMDM2aqxPHMZHjyfFloI4wSaPKBQKRUeG3Nff+LB44sPimZ02GwCP9JBfn8/3Fd+zrWIb31d8z/92/Y/Xt78OQHhouK87qa3EmGOC+QgKhULRpww5ITgcndAxzD6MYfZh/DDjhwC0elrJq85ja+VWtlZo5ZXvX8HjXXQjISzB1500PmY8To+zu1soFArFgGbIC0FXGHVGxkaPZWz0WK4YpeXaa2xtZGfVTp8wbK3cyhcHv/Cd8/x7zzMyciSjI0czKnIUoyJHkWpLRa/TB+sxFAqFwi+UEPiJxWhhavxUpsZP9dXVNNWwrXIbizYswh3hZlf1Lr4s+NLnOZgNZjIjMn3CMCpyFCMjRxIeGh6sx1AoFIojUEJwHESYIpiRPIPWPa3knJkDQJOrib21e9ldtZvd1VpZemgp7+15z3deYlhiuzhEadthtmHKe1AoFEFBCUEfYzKYyIrOIiu6ffaflJJyZzm7qnb5xGF39W6+Lvwat3QDEKoP7eQ9jI4aTbo9nWhzNDqhC9bjKBSKIYASgn5ACEGcJY44SxxnpJzhq29xt7Cvdl8ngVhZsJIP8j7wtQnRhZBkTSIxLJEkaxLJ1mTfNjEskVhLrBIKhUJxXCghCCIh+hDGRI1hTNQYX52UksqmSnZV7SK/Pp8iRxGFjkKKHEWsyF9BVVNVp2sYdcZOInG4YMSaY1WXk0Kh6BYlBAMMIQQx5hhikrueq9DY2khJQ4lPHIoairSto4jc/FwqmzqvGWTQGUiwJPiEIcmaRENDAwlVCQyzD8NsGDg50RUKRXBQQjDIsBgtZERkkBGR0eXxJlcTxQ3FnTyJNrH4uvBryp3lALz28WsAJIUlkR6ezvDw4Qy3D/ftx5pj1YxqhWKIoITgBMNkMGkv9fDhXR5vcjXx/vL3iR4VzYHaA+yv28/+2v28v+d9nB1WWQozhpFuT/ddq20/zZ5GqL7/FsxQKBSBRwnBEMNkMJEUkkROek6neiklZY1lPmE4UHuA/bX72VC6gUX7Fvna6YSOpLCkdoEIT/d5EtGmaOVFKBSDECUECkCLTbTlYZqeOL3TscbWRg7WHWR/7X7217WLxLqSdTR3WKTbYrCQakvVij21fd+WSoIlQQWtFYoBihICRY9YjBZfyo2OeKSHkoYSzYOoO0B+fT759fnk1eSxsmAlrR3WaDXoDKRYU0ixpZBqSyXNluYTiWRbsupuUiiCiBICxTGjEzrfSKQZyZ1XVXJ73JQ1lpFfn8+h+kM+kSioL2Bz2WYcrQ5fW4HmjXT0IFJsKaTZ0lRCP4WiHwiYEAgh5gE/BMqklOO7OJ4DfATs91a9L6V8NFD2KPoXvU5PojWRRGsiJyee3OmYlJLq5mqfOOTX55Nfp21X5q88YgjsE289Qaq1XSA6buMscWpCnUJxnATSI3gNeAGY302br6SUPwygDYoBiBCCKFMUUaYoJsVOOuJ4Y2ujTyByN+cSGhdKgaPAl/HVJV2+tkadkWRrcrtAWNuFIsWWouZJKBR+EDAhkFJ+KYRID9T1FScuFqOF0VGjGR01GsN+Azmn5viOuTwuShpKKHAUdOpu6qrLCSDGHHOEQKTaUkm3pxNhiujnJ1MoBiZCShm4i2tCsKibrqH3gAKgCLhbSrntKNe5BbgFID4+/qS33norQBYfGw6HA6vVGmwz/GYw2dsbW6WUNHoaqXBVtJfWCipdlVS4Kqhx1yBp/3u36WwkGBNICEkgwZhAojGRBGMCNr2tX+wNNoPJVhhc9g5EW2fNmrVRSpnd1bFgCoEd8EgpHUKIOcDzUsqRPV0zOztbbtiwoc9tPR5yc3PJyckJthl+M5js7UtbW9wtFDoKya/PZ3/tfvbV7mNvzV721uzt5ElEhkaSEZFBZkQmGeHebUSGX/MkhurPtj8YTPYORFuFEEcVgqCNGpJS1nXYXyyEeFEIESOlrAiWTYoTmxB9iG8i3MyUmb76tsl0e2v2srd2r08cFu9bTH1rva9deGg4I8JHMCKiQwkfQYw5Rk2kUwxqgiYEQogEoFRKKYUQSp971QAAHNpJREFUJwM6oLKH0xSKPqfjZLrTkk/z1betI7G3Zi/7aveRV5PHvpp9fH7gc+pafN9jsIXYfALRXNtMbV6tdj2LVixGSzAeS6Hwm0AOH30TyAFihBAFwEOAEUBK+U/gcuBWIYQLcAJXy0D2UykUvaTjOhKnJp3qq29LFb63Zq9PHPbW7mX5oeVUN1ez6JtFna5jC7FpohAWT4IlwbffJhTxYfFYjVblVSiCRiBHDV3Tw/EX0IaXKhSDCl+qcHMMpySe0unYkuVLGJM9htLGUq00dN7uqtpFpbOyU9AatPQch4tD235CmJZG3BoysIKPihMHNbNYoehDQnQhpNnTSLOnHbVNq7uVcmd5J4EoaSjxicea4jWUO8vxSE+n88JDw7U5E9YUkm3atm0/KSwJo94Y6MdTnKAoIVAo+hmj3uhLzXE0XB4Xlc5KShtLfetLFNQXUOgoZHf1blbkr+iUy6ktTUeyNdknFim2FN9ntaSpojuUECgUAxCDzuALYE+MnXjEcY/0UNZYRqGj0CcQbftritdQ3ljeqfupbe3rjp5EdUM1EWURxFpiiTXHEqIP6c9HVAwglBAoFIMQndCREJZAQlgCJ8WfdMTxFneL5kU4Ciis94qEQ5uB/X35975RT/M+nec7JzI0kjhLHLGWWOIt8cRaYrVguTnOVx9lilKexQmIEgKF4gQkRB9Ceng66eHpXR6va6ljUe4i0rLSKG/U4hXljeWUNZZR5ixjZ9XOLoPaBmEgxhJzhED4hMMcR0JYghoyO8hQQqBQDEHsIXaSQpI4Pfn0o7ZxeVxUOCs6CURZY3vZX7uftcVrO026ayMiNEKLg4Ql+eIhiWGJJFuTSbQmYg+xB/LxFL1ECYFCoegSg87g637qjsbWRiqcFZQ2llLWWEZxQzHFjmIKGwrZV7uPb4q+6bQeNoDNaCPRmthJKDoKR0RohJpX0Y8oIVAoFMeFxWghzXj0IbNt608UO4opdBRS3ODdOoopaihiQ+mGI7LGmg1mEsMSfeKQaE2k2lGNudhMrDmWWEusmoTXhyghUCgUAaXj+hNZMVldtqlrqaPIUUSRo6iTUBQ6CtlasZWa5hoA5i9pX97EbGgXhThznC+47avz7qt4Rc8oIVAoFEHHHmLHHmVnTNSYLo83tjayKHcRwycMp7yxnHKnFrcobyynzFnGtsptlBeUH9EFBRBmDCPWHOsLbLeJRtuw2VhzLDHmmCEtGEoIFArFgMditBBnjGNawrSjtpFS4mh1UO4s9wW4D9/fXLaZ8sZyWjwtR5xvNph9qUO6KtHmaGJMMUSZozDqTqxZ3EoIFArFCYEQAluIDVuIjYzwjKO2k1JS11Ln8ygqmyopd5ZT4aygwllBpVNLKLi2eG2nLLO++yCINEX6hKEr0ShqKaKkoQRbiA2LwTLgYxlKCBQKxZBCCEF4aDjhoeGMjOx+LaxmdzOVzkqfSBxeKp2VHKw7SLmzvFPKD4A/vvtHAPRCjzXEis2oiZQ9xK599opWW50txIbNaMMaYm3/HGIjzBgW8El8SggUCoXiKITqQ3vMCwXtXkalU/MuVm1aRerIVBwtDupa6v5/e2ceHMd13/nPr3vuGdw3AUigRFC0SJESCUu0LEqUZdJxShs7qdRa61Rt4tjx7la8SbzZcqwktWvvP9FWUtl4azd2VElW5axWysZWbBVLK1OmBDmOrIOUSOqgeMikSIDEReKYA8D0TL/9o3sGAxDgCWgGmt+n6tU7u/uHh+n+9jv6PZLZJEkn6fnZJO9PvV8MZ3KZS55bkKKQPLThIb6w6QvL+ScCKgSKoijXTWkr46b6m5iOT7Nz/c4rOjbn5khlU/OEouCmslOknFQxfrlvOq4VFQJFUZQyErAC1EfqqY/Ul80GXT1KURSlylEhUBRFqXKqRgiM62Jc9/IFFUVRqoyqEYLMK69w/L77OPfNb5L+2c8wuVy5TVIURakIqmaw2EokiG3dxuQPfsjEE09i19WReOABanbvIn733Vgh3Z1JUZTqpGqEIHrbbXR96y9wp6dJ/fSnJPc+R3LvXiafegorkSCxcyc1u3eR2LEDKxott7mKoigfGFUjBAWsaJTaXbuo3bULN5sl8/LLTO3dS+rH+5jasweJRkns2EHNrl0k7t+JnUiU22RFUZQVpeqEoBQrFCJx770k7r0X841vkNm/n+TevSSf+zHJvXuRYJD43XdTs3s3iU/cT6ChodwmK4qiLDtVLQSlSCBAfPt24tu30/bHf8z0wYPF7qPUiy+CbRO/605qdu+m5oEHCLS0lNtkRVGUZUGFYBHEsoht3Ups61Za/+BrzLz9jtdS2LuXoW98k6Fv/heiW7dSu3sXNbt2ldtcRVGU60KF4DKICNFNG4lu2kjLV3+P2ePHiy2F4T95hOE/eYTG7m6GXnqJ2LY+Yn3bCDQ1ldtsRVGUK0aF4CoQESLr1xNZv56Wr/w22VOnmHruOQb37GHi//4D49/9OwBCPT1E+7YVhSHY1VXx65ErilK9qBBcB6GeHpp/67d4q7eX++6+m+m332b6wAEy+w+Q3Psck9/7PgCB1lZifX1FcQj3rkOsqvmWT1GUCkeFYJmQUIjYHXcQu+MOmr70JYzrMnv8BJkD+5nef4DMgQNMPfMMAFZdnVe2bxvRbduIbtyI6AdtiqKUCRWCFUIsi8gt64ncsh4+/3mMMTiDg2T27yez3xOHVH+/VzYSIbp5M7G+bV7LYcsWrHi8vH+AoihVgwrBB4SIEOrqItTVRf1nPwtAbmyMzIHXi62Gse/8FbjfBtsmcuutRDbcQrCzi2BnJ8GuToKdnQSam7VbSalI0pOznDw0Rm1ThO5bG3VcbBWhQlBGAs3N1H5qN7Wf2g1APpVi+o2DRWFIPv8C+fPn5x0joZAnDAXX1UmoGO7CbtQbUPngcLJ5Th4c5egrQ5w5Mo5xDQAN7TE2f6KbW7a3EwzZZbZSuRwrJgQi8rfAg8CIMWbTIvkCfAv4RSAD/IYx5vWVsmc1YCcSJHbcQ2LHPcU0N5PBOXsWZ3CQ7MAAzqAXdgYGmHnrLfITE/POIdEowc41BDsLAtE1TzTs+vLtgqR8OHBdw+CxcY69PMR7b4zizOZJNIbZuvsGej/axtiZJAf3neHF/3OUl3/4Hht3dHLbfV0kGsLlNl1ZgpVsETwG/A/gu0vkfxro9d1dwLd9f0WYTmU5d2KS7lsbV9UbihWLEV63jvC6dYvm51NpTxjmuQGyg4NMHzyEOzl50fmaams5ffP/JtDeRrC9o+gH29sIdHTo+krKopwfTHH05SGOvTZMemKWUMRmXV8rt9zVzpp19YjltUSbOhOsv6udcycmOLRvgNd/9D4H955mXV8rWx7opvXG2jL/JcpCVkwIjDE/EZGeSxT5DPBdY4wBXhaRehHpMMacWwl7Th0e4/nvvosdtOj+SCNrtzTTc1szsdrVPVvHTsSxC4PSi5Cfmiq2KJwBTyDOvfkm+WSS2WPHyI2NgTHzjrHicQId7QTb2ot+sKOdQFu7ikWVkZ6c5dirwxx9ZYjzAyksS7hhYyMf/9V1rN3cTGCJlyoRYU1vA2t6G5gcnebNFwZ456WzHHt1mI6b69jyQDdrb2/BsrQbsxIo5xhBJ3CmJD7gp62IEKy/q51EY4STh8Y4eWiUU4fHQKDjpjp6tjRz05YW6ttiK3HpsmLX1mLX1hLZsKGY9m5/P3fs3AmAcRxyIyM4w8M4586RGxrGGRoiNzSEMzx85WLR3kGwo6PYLRVsa9MpsasUZzbPz/1+/4EjFzAGWm+sYcfneuntayNac3X/17qWKPf8y17u/BdrOfLSOQ6/cIZnH32LmqYIm+/v4iMfX0M4qsOV5UTMght8WU/utQj2LDFGsAd4xBjzUz++D/gDY8z+Rcp+GfgyQFtb27Ynn3zyuuwyxjAzAclBSA4aZsa99FAt1HZCTacQbeKKB11TqRSJVfSGfNX25vNYExPY4xNY4xewxyewx8exJsaxL4xjTUxgTU0hJb8lI4JbV0e+sZF8UyNuYxP5pkYv7ocJX77P+ENft2Wk1FbjGtIjMHHKkBwANwfBGNT1QH2PEK5dvjd34xqSZ+H8UUNmFKwA1N8ETb1CqGbp66zWuq0U7r///gPGmL7F8sopw4NAd0m8y0+7CGPMo8CjAH19fWan/za7XEydn+bU4TFOHhrj7NEJxo4YYrUhejY3s3ZLM10bGggElx5X6O/vZzltMsaQnc5hB61LXvdaWW57AUw2izM05HdDnfX8UnfwEDiOVxbBiI3V2Ijd0YndvoZAewdWWzt2axt2SytWcytE4+x/bT8bbrwDkze4rudM3uAW4nmDcQ1u3p0f98OFcmZB3LKFYMgiELIJhOx54UAxbBFckHa5royl6ta4Biebx5n1XC6bx5nx44X0mfy8Ms5sntxsnnzOJRiyCUYCBMM2wYhNKGJ74XDAi/vpwbBXJhSxsYPWJV9m+vv72bSuj6OvDHH81SHSk1lC0QAb7mrhlu3tdNw81++/Uoy8P8Wh589wYv8IF44b1m5uYssD3azprb/I9pX43a4Uq8lWKK8QPA18RUSexBsknlyp8YHLUdsUZfP93Wy+v5uZtMPpt89z8tAYx/cP885PzxII29xwayM3bWnmxtuaicSD13W9vOOSmpgldWGG5PiM7/vxC57vzOZBoLYpQkNHnIa22Dz/em24VlzXkLoww+TINBMjGc8fzZC6MEs+5/oP6nbcfCuuuwW32eA2GMyGwsN6kZPm8DoGBwoJY77z+PneA8tjvIBlC5ZIUSiuFtsW7AAEAhCwDLa42JLHNjkskyM9neXvn3mWnLHJuRa5vJDLwdVukR0IFx70NrYtnkDMeM51r8xusaQoCoVzlYrF6eMub0+86vX7b2rinrva6dnctCIvH0vRemMtu76wkbt/eR1vvjjA2z85y8lDYzR3J9jyQDe929qwg/rdzEqzktNHnwB2As0iMgD8ZyAIYIz5DvAM3tTRE3jTR7+wUrZcDZF4kPV3trP+znbyjsvgsfHiuMLP3xhFLGHNujrWbmlh7ZZmapvnb2tpjGE66ZAanyF5YYbUhVnPH597yGemshddN1oTpKYxQkNbjO6PNJBoiODM5BgfzjB+LsPAkXHyOXde+Yb2OA3tsaJf3x6jpiFy3W9xxhjSE1n/QZ9hYmS66E+NTs+zIxCyqGuNUdscwQ5aWJZ4zhbEtophyxLEnsuzbEEswbYtxBLEAmanMZMTuJMTmIlx3PHzTJw5TU3QwqRTkExiUlMwnUFMHjFuie8ibh4L1xtAr40TqK0lUJsgWFtDoK6WQH0ddn0dVm0tGMhNpcimMjjJDNn0LE56llwmizOdxZnNkct6b+RODvKu4Foh8nbQ98Ml4RBZK4RrBbFch/zICAE3Szg/i1102ZLwLDZ5gkG8lkjYIhgOEooGCEZDBGNh7EQMKxbHisewojEkGESCAQgEMXaQHAFy4vsm4AuPTc5Y5PIWOVdwcuDkPCFysgbHMTiOy0x6Gmcmj2XDjs+tp7ev9ar7/ZebeH2Y7Z+5mb5P93D0lSEOPT/AvseO8LOn3mPTfZ1surezrPZ92FnRMYKVoK+vz+zff9EwwopjXMPI6SQnD41y8tAYF86mAWjqjJMLpElEGkhdmCE1PjvvQQnew7KmMUKiMUKiIeyFGyLUNIZJNHhpS82+KOC6huT5GcaH0owPZRgfSjMxlOHCUJrZ9NzrZiBkUd8Wu1gkWmPFN6v+/n7uu+8+ppPO3MN+eO5hPzmaIZed+xvsgEVda5S6lij1rTHqWqPUt3nnjNWFVvQDtsWa2MZxyCeT5CcmyU9OkJ+cxJ2cJD856actdBO4E5Pkp6YuGvQG7yM9K5HAisexEgnseLwY9pwXtxMJrHiiWNZOlJSJx7GiUV7ct497tm3DTadxMxnPFcLpNG56kbSF5UrCZmZmZSrWtnEDAUL19Vg1Cex4AqumxgsnarBqarBrEliJGq9OCuGaBHaNnx+Pr9iEAGMMZ45c4NC+AU6/fR6xBCtoiMbDBIJz3XZ20JrftRe0LureK/rBuXjx2KD3ImJZUnwhmQt7rUexvNbV1fzOK7FrSESWHCNQIbhGJkczfkthjNHBCZra60g0hqlp8B74NY1hz2+IEI4HVuxhaYxhJuXMCcS5DOPDacbPZUhemHuIiEBts/cwHzl3gfy0jTOTL+ZbllDbEqW+NUpda6zo17VGl6WVca0s5w1lXBc3mSQ/OQmW5T2843GsZXyYLft4US6HOzODcRyM44DjYHK5YrwYzhbCWa/cwjLZBeUdh9PHj7OmoR43mcJNJcknU179pFO4ydQViZCEw54oJBJz4hGP+60Zv0VTjMfm6jx2cVjC4UXvkwvn0hx/bZj3jp2iraWdXNYl57heiy2bx8m65P24k82T8+PLjQhFgfBEwotbticSpUIyMztDPB4rtnwL4lL0bZkvOPaCMgviln/uzlsa6Lmt+RrtX1oIdM7WNVLXEuP2T97A7Z+8wb/5t5XFDhEhWhMiWhNiTe/8PZWdbJ6JoTlhGB/KMDU2TSAMvVs6qG+be+jXNEaw7A93X6xYFnZdHXZdXblNuWIkEFixbzbe6e9nzSVEy2Sz5NNpTxySSdxU2heM5Jx4pFIXCUludGxei8f4kwQui23PCcQCoeiKxwmeP0/HDd1IKOR3lQWReBBpKIkHg14rJRDEtYLkrQB5K4QrAfLY5PF9Y5E3NnkrgESiGCMY12CMwbh4kw7M3ESDwgQE43q9A64ppDOvTCFv6NwMjc2JkuPmT27IOy5OIW5KJz3MxUuPK0yWCITsaxaCS6FC8CEmGLJpuaGGlhtq5qX39/dz787FP0BTlAISChEIhaCh4fKFL4HJZouikE+nMZmMJzDFrjLfn9eFNhd2xsdxMxlCySTJd9+da+k4DuTzlzfgEtgAwSCBlmaCLa0E2to819pCsK2NQGsbgbZWgq1tV7UicH//KDt3XjRrvmJRIVAUZUWRUAg7FMKur+d65rotOl6Uz88TBpPNLog7XndZ1lm0nJvJkBsZITc8jDMyzOyJE6Rfegk3lbro+lYiMScSrQXBaPWEoiAgTU1IYOnHqtdNl/XEMZsFx8HN+vZls76t851bDDtEbv0Isa1br6MWF0eFQFGUVYvYNmLbEIks63nddBpneITcyHDxy/vcsCcYuZER0q++Sm509OJ5wZZFoKmJJhGOB+y5B7zvFp8/feU0femLKgSKoigfBFY8TvimtYRvWrtkGeO65M+fxxkZ8URiZLgoGKnT7xPv7PLGM+Y5bwzDWpgenF/GCocXOTaEFVuZZXBUCBRFUa4BsSwCLS0EWlpg48Z5ecdK1vNaDXy4p4koiqIol0WFQFEUpcpRIVAURalyVAgURVGqHBUCRVGUKkeFQFEUpcpRIVAURalyVAgURVGqnFW3DLWIjALvl9uOBTRTuqVW5bOa7F1NtsLqsnc12Qqry95KtPVGY0zLYhmrTggqERHZv9Q635XIarJ3NdkKq8ve1WQrrC57V5OtoF1DiqIoVY8KgaIoSpWjQrA8PFpuA66S1WTvarIVVpe9q8lWWF32riZbdYxAURSl2tEWgaIoSpWjQqAoilLlqBBcJSLSLSIviMg7IvK2iPyun94oIs+JyHHfv74dv5cREbFF5A0R2ePH14rIKyJyQkT+XkRC5baxgIjUi8j3RORdETkiIh+r1LoVka/6v4G3ROQJEYlUUt2KyN+KyIiIvFWStmhdisd/9+0+LCLLvx/i1dv6p/7v4LCI/KOI1JfkPezbelREPvVB2rqUvSV5vy8iRkSa/XhZ6/ZKUCG4enLA7xtjbgW2A78tIrcCXwf2GWN6gX1+vFL4XeBISfy/Av/NGLMOGAe+WBarFudbwLPGmA3AFjy7K65uRaQT+B2gzxizCbCBh6isun0M+IUFaUvV5aeBXt99Gfj2B2Rjgce42NbngE3GmM3AMeBhAP9+ewjY6B/zlyJif3CmAovbi4h0A7uB0yXJ5a7by2OMUXcdDvghsAs4CnT4aR3A0XLb5tvShXfDfwLYAwjeF48BP/9jwI/KbadvSx1wEn8SQ0l6xdUt0AmcARrxtnzdA3yq0uoW6AHeulxdAn8F/KvFypXL1gV5vww87ocfBh4uyfsR8LFy162f9j28F5hTQHOl1O3lnLYIrgMR6QHuAF4B2owx5/ysIaCtTGYt5C+ArwGuH28CJowxOT8+gPdQqwTWAqPA//K7sv5aROJUYN0aYwaBP8N78zsHTAIHqNy6LbBUXRaErUCl2f6bwP/zwxVpq4h8Bhg0xhxakFWR9paiQnCNiEgC+D7we8aYqdI848l+2eflisiDwIgx5kC5bblCAsBW4NvGmDuANAu6gSqobhuAz+CJ1xogziJdBZVMpdTl5RCRP8Lrkn283LYshYjEgD8E/lO5bbkWVAiuAREJ4onA48aYp/zkYRHp8PM7gJFy2VfCx4FfEpFTwJN43UPfAupFJOCX6QIGy2PeRQwAA8aYV/z49/CEoRLr9pPASWPMqDHGAZ7Cq+9KrdsCS9XlINBdUq4ibBeR3wAeBH7NFy6oTFtvxnspOOTfb13A6yLSTmXaOw8VgqtERAT4G+CIMebPS7KeBn7dD/863thBWTHGPGyM6TLG9OANrj1vjPk14AXgV/1iFWErgDFmCDgjIrf4SQ8A71CBdYvXJbRdRGL+b6Jga0XWbQlL1eXTwL/2Z7hsByZLupDKgoj8Al635i8ZYzIlWU8DD4lIWETW4g3CvloOGwsYY940xrQaY3r8+20A2Or/piuubi+i3IMUq80B9+A1pw8DB333i3h97/uA48CPgcZy27rA7p3AHj98E96NcwL4ByBcbvtK7Lwd2O/X7w+AhkqtW+CbwLvAW8DfAeFKqlvgCbzxCwfvwfTFpeoSbxLB/wTeA97Emw1VbltP4PWtF+6z75SU/yPf1qPApyuhbhfkn2JusLisdXslTpeYUBRFqXK0a0hRFKXKUSFQFEWpclQIFEVRqhwVAkVRlCpHhUBRFKXKUSFQqg4Recn3e0Tk88t87j9c7FqKUsno9FGlahGRncB/NMY8eBXHBMzcWkKL5aeMMYnlsE9RPii0RaBUHSKS8oOPADtE5KC/t4Dtr4H/mr9u/L/xy+8UkX8Skafxvh5GRH4gIgf8/Qi+7Kc9AkT98z1eei3/q9I/FW/vgjdF5HMl5+6XuT0YHve/VEZEHhFv34vDIvJnH2QdKdVF4PJFFOVDy9cpaRH4D/RJY8xHRSQM/LOI7PXLbsVbG/+kH/9NY8wFEYkCr4nI940xXxeRrxhjbl/kWr+C99X0FqDZP+Ynft4deGvrnwX+Gfi4iBzBW3p5gzHGlG7KoijLjbYIFGWO3XhrwhzEW1q8CW8dG4BXS0QA4HdE5BDwMt6CYr1cmnuAJ4wxeWPMMPAi8NGScw8YY1y8pRR68Ja1ngH+RkR+Bcgsck5FWRZUCBRlDgH+vTHmdt+tNcYUWgTpYiFvbOGTeJuhbAHeACLXcd3ZknAeb2ObHHAn3gqsDwLPXsf5FeWSqBAo1UwSqCmJ/wj4d/4y44jIen9jnIXUAePGmIyIbMDbsrSAUzh+Af8EfM4fh2gB7uUSK2b6+13UGWOeAb6K16WkKCuCjhEo1cxhIO938TyGt1dDD9468oK3W9pnFznuWeDf+v34R/G6hwo8ChwWkdeNt+R3gX/E27ryEN7qtV8zxgz5QrIYNcAPRSSC11L5D9f2JyrK5dHpo4qiKFWOdg0piqJUOSoEiqIoVY4KgaIoSpWjQqAoilLlqBAoiqJUOSoEiqIoVY4KgaIoSpXz/wGEw0rfULt4lAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "as you can see from the graph above:\n",
        "\n",
        "for μ=0.01, we get almost a stright line - the learning rate is to small!\n",
        "\n",
        "for μ=0.05,0.1,0.5 we got a desired learning rate - a monotonic descending slope.\n",
        "\n",
        "for μ=1.2, the learning rate was a bit high, minor jumps to higer value, but overall we got a small cost.\n",
        "\n",
        "for μ=5, the step size is to big and we jump in and out from local minima.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355956fe-4814-41fe-e907-3d6b7dae135c"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "# we wiil iterate on some mu and batch size values.\n",
        "# we will run the simulation on a fixed value for 'max_iterations' = 150\n",
        "wb_dict = {}\n",
        "\n",
        "for mu in [0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3]:\n",
        "  for batch_size in [100, 150, 200, 250, 300, 350, 400, 450]:\n",
        "      w,b,cost_vec= run_gradient_descent(w0, b0, mu=mu, batch_size=batch_size)\n",
        "      print(f'Done! mu = {mu}, batch size = {batch_size}')\n",
        "      wb_dict[f'{cost_vec[-1]}'] = b,w,mu,batch_size \n",
        "i=0\n",
        "for k in sorted(wb_dict):\n",
        "  if i == 1:\n",
        "    break\n",
        "  else:\n",
        "    opt_w = wb_dict[k][0]\n",
        "    opt_b = wb_dict[k][1]\n",
        "    opt_mu = wb_dict[k][2]\n",
        "    opt_batchSize = wb_dict[k][3]\n",
        "    i+=1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! mu = 0.08, batch size = 100\n",
            "Done! mu = 0.08, batch size = 150\n",
            "Done! mu = 0.08, batch size = 200\n",
            "Done! mu = 0.08, batch size = 250\n",
            "Done! mu = 0.08, batch size = 300\n",
            "Done! mu = 0.08, batch size = 350\n",
            "Done! mu = 0.08, batch size = 400\n",
            "Done! mu = 0.08, batch size = 450\n",
            "Done! mu = 0.1, batch size = 100\n",
            "Done! mu = 0.1, batch size = 150\n",
            "Done! mu = 0.1, batch size = 200\n",
            "Done! mu = 0.1, batch size = 250\n",
            "Done! mu = 0.1, batch size = 300\n",
            "Done! mu = 0.1, batch size = 350\n",
            "Done! mu = 0.1, batch size = 400\n",
            "Done! mu = 0.1, batch size = 450\n",
            "Done! mu = 0.12, batch size = 100\n",
            "Done! mu = 0.12, batch size = 150\n",
            "Done! mu = 0.12, batch size = 200\n",
            "Done! mu = 0.12, batch size = 250\n",
            "Done! mu = 0.12, batch size = 300\n",
            "Done! mu = 0.12, batch size = 350\n",
            "Done! mu = 0.12, batch size = 400\n",
            "Done! mu = 0.12, batch size = 450\n",
            "Done! mu = 0.14, batch size = 100\n",
            "Done! mu = 0.14, batch size = 150\n",
            "Done! mu = 0.14, batch size = 200\n",
            "Done! mu = 0.14, batch size = 250\n",
            "Done! mu = 0.14, batch size = 300\n",
            "Done! mu = 0.14, batch size = 350\n",
            "Done! mu = 0.14, batch size = 400\n",
            "Done! mu = 0.14, batch size = 450\n",
            "Done! mu = 0.16, batch size = 100\n",
            "Done! mu = 0.16, batch size = 150\n",
            "Done! mu = 0.16, batch size = 200\n",
            "Done! mu = 0.16, batch size = 250\n",
            "Done! mu = 0.16, batch size = 300\n",
            "Done! mu = 0.16, batch size = 350\n",
            "Done! mu = 0.16, batch size = 400\n",
            "Done! mu = 0.16, batch size = 450\n",
            "Done! mu = 0.18, batch size = 100\n",
            "Done! mu = 0.18, batch size = 150\n",
            "Done! mu = 0.18, batch size = 200\n",
            "Done! mu = 0.18, batch size = 250\n",
            "Done! mu = 0.18, batch size = 300\n",
            "Done! mu = 0.18, batch size = 350\n",
            "Done! mu = 0.18, batch size = 400\n",
            "Done! mu = 0.18, batch size = 450\n",
            "Done! mu = 0.2, batch size = 100\n",
            "Done! mu = 0.2, batch size = 150\n",
            "Done! mu = 0.2, batch size = 200\n",
            "Done! mu = 0.2, batch size = 250\n",
            "Done! mu = 0.2, batch size = 300\n",
            "Done! mu = 0.2, batch size = 350\n",
            "Done! mu = 0.2, batch size = 400\n",
            "Done! mu = 0.2, batch size = 450\n",
            "Done! mu = 0.22, batch size = 100\n",
            "Done! mu = 0.22, batch size = 150\n",
            "Done! mu = 0.22, batch size = 200\n",
            "Done! mu = 0.22, batch size = 250\n",
            "Done! mu = 0.22, batch size = 300\n",
            "Done! mu = 0.22, batch size = 350\n",
            "Done! mu = 0.22, batch size = 400\n",
            "Done! mu = 0.22, batch size = 450\n",
            "Done! mu = 0.24, batch size = 100\n",
            "Done! mu = 0.24, batch size = 150\n",
            "Done! mu = 0.24, batch size = 200\n",
            "Done! mu = 0.24, batch size = 250\n",
            "Done! mu = 0.24, batch size = 300\n",
            "Done! mu = 0.24, batch size = 350\n",
            "Done! mu = 0.24, batch size = 400\n",
            "Done! mu = 0.24, batch size = 450\n",
            "Done! mu = 0.26, batch size = 100\n",
            "Done! mu = 0.26, batch size = 150\n",
            "Done! mu = 0.26, batch size = 200\n",
            "Done! mu = 0.26, batch size = 250\n",
            "Done! mu = 0.26, batch size = 300\n",
            "Done! mu = 0.26, batch size = 350\n",
            "Done! mu = 0.26, batch size = 400\n",
            "Done! mu = 0.26, batch size = 450\n",
            "Done! mu = 0.28, batch size = 100\n",
            "Done! mu = 0.28, batch size = 150\n",
            "Done! mu = 0.28, batch size = 200\n",
            "Done! mu = 0.28, batch size = 250\n",
            "Done! mu = 0.28, batch size = 300\n",
            "Done! mu = 0.28, batch size = 350\n",
            "Done! mu = 0.28, batch size = 400\n",
            "Done! mu = 0.28, batch size = 450\n",
            "Done! mu = 0.3, batch size = 100\n",
            "Done! mu = 0.3, batch size = 150\n",
            "Done! mu = 0.3, batch size = 200\n",
            "Done! mu = 0.3, batch size = 250\n",
            "Done! mu = 0.3, batch size = 300\n",
            "Done! mu = 0.3, batch size = 350\n",
            "Done! mu = 0.3, batch size = 400\n",
            "Done! mu = 0.3, batch size = 450\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-74c8ff09cf8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Done! mu = {mu}, batch size = {batch_size}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mwb_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{cost_vec[-1]}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mopt_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_batchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'we got the lowest value for the cost(if the NN converges - that the reason for taking the last element) with \\n mu={opt_mu} and batch size = {opt_batchSize} '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "according to the previus question, we wanted to check values in the area of 0.1-0.3.\n",
        "after runnig the simulation, we got mu=0.3 and batch size=400.\n",
        "the fact that we got 0.3 as the optimal result might be a reason to check higher values as well - up to 0.7. hope we will have time to do so.\n",
        "the results that optimial batch size is 400 is intersting. we expect to get the optimal as the highest value we put on the list(450), in that way our model will learn on more sampels, the fact that less sampels achive better results can be due to over fitting. althought, even with 450 as batch size, and our static chose of max iteration to be 100, in the optimial scenrio we went over 45,000 samples - 1/10 from our sampels! \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI",
        "outputId": "f4353913-579d-45bb-8984-2309be596bb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "train_acc = get_accuracy(pred(opt_w,opt_b,train_norm_xs), train_ts)\n",
        "val_acc = get_accuracy(pred(opt_w,opt_b, val_norm_xs ), val_ts)\n",
        "test_acc = get_accuracy(pred(opt_w,opt_b, test_norm_xs ), test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7334469381095682  val_acc =  0.7363  test_acc =  0.7283362386209568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "as you can see from the results, we got similiar accurecy precent for each one of the data sets: train, validation and test.\n",
        "althought, there are some minor diffrences we should pay attention to:\n",
        "1. the validation results are the highest - due to optimazing over the validation cost on several hyperparamter options - mu and batch size.\n",
        "2. the test results are the lowest - the cause for it might be due to several reasons. the main reson we can think about is after pretition, some very strange data is in the test set and not in the train, such as a musician that made a comeback :)\n",
        "3. ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ",
        "outputId": "3e0e4761-1c2f-44d0-cc5f-e02f913ca304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.linear_model as lm\n",
        "\n",
        "model = lm.LogisticRegression()\n",
        "model.fit(X=train_norm_xs, y=train_ts)\n",
        "\n",
        "pred_train = model.predict(train_norm_xs)\n",
        "pred_val = model.predict(val_norm_xs)\n",
        "pred_test = model.predict(test_norm_xs)\n",
        "\n",
        "train_acc = get_accuracy(pred_train, train_ts)\n",
        "val_acc = get_accuracy(pred_val, val_ts)\n",
        "test_acc = get_accuracy(pred_test, test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.732559854005777  val_acc =  0.73546  test_acc =  0.7265736974627155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see, the results are similiar, we can calculate the relative error:\n",
        "\n",
        "relative error of the train:\n",
        "$$\\frac{ourModel - sklearnModel}{ourModel} = 0.12\\% $$\n",
        "relative error of the validation:\n",
        "$$\\frac{ourModel - sklearnModel}{ourModel} = 0.11\\% $$\n",
        "relative error of the test:\n",
        "$$\\frac{ourModel - sklearnModel}{ourModel} = 0.24\\% $$"
      ],
      "metadata": {
        "id": "KCwXIcid2mt-"
      }
    }
  ]
}