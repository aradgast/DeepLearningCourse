{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx3i2Op-6X5n"
      },
      "source": [
        "# Assignment 2: Word Prediction\n",
        "\n",
        "**Deadline**: Sunday, December 11th, by 8pm.\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook as well as the ipynb file. \n",
        "\n",
        " \n",
        "\n",
        "In this assignment, we will make a neural network that can predict the next word\n",
        "in a sentence given the previous three.  \n",
        "In doing this prediction task, our neural networks will learn about *words* and about\n",
        "how to represent words. We'll explore the *vector representations* of words that our\n",
        "model produces, and analyze these representations.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that you properly explain what you are doing and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zdEvcdO6X5s"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQYtUQTH6X5t"
      },
      "source": [
        "## Question 1. Data (18%)\n",
        "\n",
        "With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like. Download the file\n",
        "`raw_sentences.txt` from the course page on Moodle and upload it to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eWXHhCe6X5t"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hteg6bwv6X5t"
      },
      "source": [
        "Find the path to `raw_sentences.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMsGfFi6X5u"
      },
      "source": [
        "file_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/raw_sentences.txt' # TODO - UPDATE ME!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD5uXTle6X5u"
      },
      "source": [
        "The following code reads the sentences in our file, split each sentence into\n",
        "its individual words, and stores the sentences (list of words) in the\n",
        "variable `sentences`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75NXJO_T6X5v"
      },
      "source": [
        "sentences = []\n",
        "for line in open(file_path):\n",
        "    words = line.split()\n",
        "    sentence = [word.lower() for word in words]\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbP0-e_U6X5v"
      },
      "source": [
        "There are 97,162 sentences in total, and \n",
        "these sentences are composed of 250 distinct words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLUp8rZT6X5v"
      },
      "source": [
        "vocab = set([w for s in sentences for w in s])\n",
        "print(len(sentences)) # 97162\n",
        "print(len(vocab)) # 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB77tJrU6X5v"
      },
      "source": [
        "We'll separate our data into training, validation, and test.\n",
        "We'll use `10,000 sentences for test, 10,000 for validation, and\n",
        "the rest for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJQRB6RJ6X5v"
      },
      "source": [
        "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUj7fsi06X5v"
      },
      "source": [
        "### Part (a) -- 3%\n",
        "\n",
        "**Display** 10 sentences in the training set.\n",
        "**Explain** how punctuations are treated in our word representation, and how words\n",
        "with apostrophes are represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90AmLcpF6X5w"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swyMJHYN-Taa"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2erKpOJ6X5w"
      },
      "source": [
        "### Part (b) -- 4%\n",
        "\n",
        "**Print** the 10 most common words in the vocabulary and how often does each of these\n",
        "words appear in the training sentences. Express the second quantity as a percentage\n",
        "(i.e. number of occurences of the  word / total number of words in the training set).\n",
        "\n",
        "These are useful quantities to compute, because one of the first things a machine learning model will learn is to predict the **most common** class. Getting a sense of the\n",
        "distribution of our data will help you understand our model's behaviour.\n",
        "\n",
        "You can use Python's `collections.Counter` class if you would like to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqSZO_a36X5w"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CHlVRI6X5w"
      },
      "source": [
        "### Part (c) -- 11%\n",
        "\n",
        "Our neural network will take as input three words and predict the next one. Therefore, we need our data set to be comprised of seuqnces of four consecutive words in a sentence, referred to as *4grams*. \n",
        "\n",
        "**Complete** the helper functions `convert_words_to_indices` and\n",
        "`generate_4grams`, so that the function `process_data` will take a \n",
        "list of sentences (i.e. list of list of words), and generate an \n",
        "$N \\times 4$ numpy matrix containing indices of 4 words that appear\n",
        "next to each other, where $N$ is the number of 4grams (sequences of 4 words appearing one after the other) that can be found in the complete list of sentences. Examples of how these functions should operate are detailed in the code below. \n",
        "\n",
        "You can use the defined `vocab`, `vocab_itos`,\n",
        "and `vocab_stoi` in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUZsxdHk6X5w"
      },
      "source": [
        "# A list of all the words in the data set. We will assign a unique \n",
        "# identifier for each of these words.\n",
        "vocab = sorted(list(set([w for s in train for w in s])))\n",
        "# A mapping of index => word (string)\n",
        "vocab_itos = dict(enumerate(vocab))\n",
        "# A mapping of word => its index\n",
        "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
        "\n",
        "def convert_words_to_indices(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of list of words)\n",
        "    and returns a new list with the same structure, but where each word\n",
        "    is replaced by its index in `vocab_stoi`.\n",
        "\n",
        "    Example:\n",
        "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'], ['other', 'one', 'since', 'yesterday'], ['you']])\n",
        "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n",
        "def generate_4grams(seqs):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists) and returns\n",
        "    a new list containing the 4-grams (four consequentively occuring words)\n",
        "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
        "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
        "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
        "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
        "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n",
        "def process_data(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists), and generates an\n",
        "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
        "    \"\"\"\n",
        "    indices = convert_words_to_indices(sents)\n",
        "    fourgrams = generate_4grams(indices)\n",
        "    return np.array(fourgrams)\n",
        "\n",
        "# We can now generate our data which will be used to train and test the network\n",
        "train4grams = process_data(train)\n",
        "valid4grams = process_data(valid)\n",
        "test4grams = process_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rv-6sNm6X5x"
      },
      "source": [
        "## Question 2. A Multi-Layer Perceptron (44%)\n",
        "\n",
        "In this section, we will build a two-layer multi-layer perceptron. \n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15uMLB-YsMHSOr0EQfTRhWd4o9enIOjUe\">\n",
        "\n",
        "Since the sentences in the data are comprised of $250$ distinct words, our task boils down to claissfication where the label space $\\mathcal{S}$ is of cardinality $|\\mathcal{S}|=250$ while our input, which is comprised of a combination of three words, is treated as a vector of size $750\\times 1$ (i.e., the concatanation of three one-hot $250\\times 1$ vectors).\n",
        "\n",
        "The following function `get_batch` will take as input the whole dataset and output a single batch for the training. The output size of the batch is explained below.\n",
        "\n",
        "**Implement** yourself a function `make_onehot` which takes the data in index notation and output it in a onehot notation.\n",
        "\n",
        "Start by reviewing the helper function, which is given to you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqTLOoJ6X5x"
      },
      "source": [
        "def make_onehot(data):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the index notation into its corresponding onehot\n",
        "    notation. Remember, the function should work for both xt and st. \n",
        "     \n",
        "    input - vector with shape D (1D or 2D)\n",
        "    output - vector with shape (D,250)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Write your code here\n",
        "\n",
        "def get_batch(data, range_min, range_max, onehot=True):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the form of 4-grams into input and output\n",
        "    data and return the training data (xt, st) where:\n",
        "     - `xt` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n",
        "     - `st` is either\n",
        "            - a numpy array of shape [batch_size, 250] if onehot is True,\n",
        "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
        "\n",
        "    Preconditions:\n",
        "     - `data` is a numpy array of shape [N, 4] produced by a call\n",
        "        to `process_data`\n",
        "     - range_max > range_min\n",
        "    \"\"\"\n",
        "    xt = data[range_min:range_max, :3]\n",
        "    xt = make_onehot(xt)\n",
        "    st = data[range_min:range_max, 3]\n",
        "    if onehot:\n",
        "        st = make_onehot(st).reshape(-1, 250)\n",
        "    return xt, st\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvLuZpH-6X52"
      },
      "source": [
        "### Part (a) -- 8%\n",
        "\n",
        "We build the model in PyTorch. Since PyTorch uses automatic\n",
        "differentiation, we only need to write the *forward pass* of our\n",
        "model. \n",
        "\n",
        "**Complete** the `forward` function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMzWMUB16X52"
      },
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    def __init__(self, num_hidden=400):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(750, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "    def forward(self, inp):\n",
        "        inp = inp.reshape([-1, 750])\n",
        "        # TODO: complete this function \n",
        "        # Note that we will be using the nn.CrossEntropyLoss(), which computes the softmax operation internally, as loss criterion\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "114NF7If6X52"
      },
      "source": [
        "### Part (b) -- 10%\n",
        "\n",
        "We next  train the PyTorch model using the Adam optimizer and the cross entropy loss.\n",
        "\n",
        "**Complete** the function `run_pytorch_gradient_descent`, and use it to train your PyTorch MLP model.\n",
        "\n",
        "**Obtain** a training accuracy of at least 35% while changing only the hyperparameters of the train function.\n",
        "\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY70vUCZ6X52"
      },
      "source": [
        "def estimate_accuracy_torch(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        \n",
        "        # forward pass prediction\n",
        "        y = model(torch.Tensor(xt))\n",
        "        y = y.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "        pred = np.argmax(y, axis=1)\n",
        "        correct += np.sum(pred == st)\n",
        "        N += st.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def run_pytorch_gradient_descent(model,\n",
        "                                 train_data=train4grams,\n",
        "                                 validation_data=valid4grams,\n",
        "                                 batch_size=100,\n",
        "                                 learning_rate=0.001,\n",
        "                                 weight_decay=0,\n",
        "                                 max_iters=1000,\n",
        "                                 checkpoint_path=None):\n",
        "    \"\"\"\n",
        "    Train the PyTorch model on the dataset `train_data`, reporting\n",
        "    the validation accuracy on `validation_data`, for `max_iters`\n",
        "    iteration.\n",
        "\n",
        "    If you want to **checkpoint** your model weights (i.e. save the\n",
        "    model weights to Google Drive), then the parameter\n",
        "    `checkpoint_path` should be a string path with `{}` to be replaced\n",
        "    by the iteration count:\n",
        "\n",
        "    For example, calling \n",
        "\n",
        "    >>> run_pytorch_gradient_descent(model, ...,\n",
        "            checkpoint_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-{}.pk')\n",
        "\n",
        "    will save the model parameters in Google Drive every 500 iterations.\n",
        "    You will have to make sure that the path exists (i.e. you'll need to create\n",
        "    the folder Intro_to_Deep_Learning, mlp, etc...). Your Google Drive will be populated with files:\n",
        "\n",
        "    - /content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-500.pk\n",
        "    - /content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-1000.pk\n",
        "    - ...\n",
        "\n",
        "    To load the weights at a later time, you can run:\n",
        "\n",
        "    >>> model.load_state_dict(torch.load('/content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-500.pk'))\n",
        "\n",
        "    This function returns the training loss, and the training/validation accuracy,\n",
        "    which we can use to plot the learning curve.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "\n",
        "    n = 0 # the number of iterations\n",
        "    while True:\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xt, st = get_batch(train_data, i, i + batch_size, onehot=False)\n",
        "\n",
        "            # convert from numpy arrays to PyTorch tensors\n",
        "            xt = torch.Tensor(xt)\n",
        "            st = torch.Tensor(st).long()\n",
        "\n",
        "            # zs = ...                 # compute prediction logit\n",
        "            # loss =                   # compute the total loss\n",
        "            # ...                      # compute updates for each parameter\n",
        "            # ...                      # make the updates for each parameter\n",
        "            # ...                      # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)  # compute *average* loss\n",
        "\n",
        "            if n % 500 == 0:\n",
        "                iters_sub.append(n)\n",
        "                train_cost = float(loss.detach().numpy())\n",
        "                train_acc = estimate_accuracy_torch(model, train_data)\n",
        "                train_accs.append(train_acc)\n",
        "                val_acc = estimate_accuracy_torch(model, validation_data)\n",
        "                val_accs.append(val_acc)\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "                if (checkpoint_path is not None) and n > 0:\n",
        "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBq-1F86X52"
      },
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "# learning_curve_info = run_pytorch_gradient_descent(pytorch_mlp, ...)\n",
        "\n",
        "\n",
        "# plot_learning_curve(*learning_curve_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXqpx0v6X52"
      },
      "source": [
        "### Part (c) -- 10%\n",
        "**Write** a function `make_prediction` that takes as parameters\n",
        "a PyTorchMLP model and sentence (a list of words), and produces\n",
        "a prediction for the next word in the sentence.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2jOK7B26X52"
      },
      "source": [
        "def make_prediction_torch(model, sentence):\n",
        "    \"\"\"\n",
        "    Use the model to make a prediction for the next word in the\n",
        "    sentence using the last 3 words (sentence[:-3]). You may assume\n",
        "    that len(sentence) >= 3 and that `model` is an instance of\n",
        "    PYTorchMLP.\n",
        "\n",
        "    This function should return the next word, represented as a string.\n",
        "\n",
        "    Example call:\n",
        "    >>> make_prediction_torch(pytorch_mlp, ['you', 'are', 'a'])\n",
        "    \"\"\"\n",
        "    global vocab_stoi, vocab_itos\n",
        "\n",
        "    #  Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHvIKjYg6X53"
      },
      "source": [
        "### Part (d) -- 10%\n",
        "\n",
        "Use your code to predict what the next word should be in each\n",
        "of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "Do your predictions make sense?\n",
        "\n",
        "In many cases where you overfit the model can either output the same results for all inputs or just memorize the dataset. \n",
        "\n",
        "**Print** the output for all of these sentences and \n",
        "**Write** below if you encounter these effects or something else which indicates overfitting, if you do train again with better hyperparameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzhshY56X53"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTfh4MwjAlGB"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4QBM0fo6X53"
      },
      "source": [
        "### Part (e) -- 6%\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq31oqDR6X53"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlr7C8yg6X53"
      },
      "source": [
        "## Question 3. Learning Word Embeddings (24 %)\n",
        "\n",
        "In this section, we will build a slightly different model with a different\n",
        "architecture. In particular, we will first compute a lower-dimensional\n",
        "*representation* of the three words, before using a multi-layer perceptron.\n",
        "\n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=16lXygLTSuRgOCj6UWK0vHkSoyRJWfMSZ\" />\n",
        " \n",
        "\n",
        "This model has 3 layers instead of 2, but the first layer of the network\n",
        "is **not** fully-connected. Instead, we compute the representations of each\n",
        "of the three words **separately**. In addition, the first layer of the network\n",
        "will not use any biases. The reason for this will be clear in question 4.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0td55ll6X54"
      },
      "source": [
        "### Part (a) -- 10%\n",
        "\n",
        "The PyTorch model is implemented for you. Use \n",
        "`run_pytorch_gradient_descent` to train\n",
        "your PyTorch MLP model to obtain a training accuracy of at least 38%.\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWlfclh6X54"
      },
      "source": [
        "class PyTorchWordEmb(nn.Module):\n",
        "    def __init__(self, emb_size=100, num_hidden=300, vocab_size=250):\n",
        "        super(PyTorchWordEmb, self).__init__()\n",
        "        self.word_emb_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "        self.fc_layer1 = nn.Linear(emb_size * 3, num_hidden)\n",
        "        self.fc_layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, inp):\n",
        "        embeddings = torch.relu(self.word_emb_layer(inp))\n",
        "        embeddings = embeddings.reshape([-1, self.emb_size * 3])\n",
        "        hidden = torch.relu(self.fc_layer1(embeddings))\n",
        "        return self.fc_layer2(hidden)\n",
        "\n",
        "# pytorch_wordemb= PyTorchWordEmb()\n",
        "\n",
        "# result = run_pytorch_gradient_descent(pytorch_wordemb,\n",
        "#                                       max_iters=20000,\n",
        "#                                       ...)\n",
        "\n",
        "# plot_learning_curve(*result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oekGJRad6X54"
      },
      "source": [
        "### Part (b) -- 10%\n",
        "\n",
        "Use the function `make_prediction` that you wrote earlier to predict what the next word should be in each of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "How do these predictions compared to the previous model?\n",
        "\n",
        "**Print** the output for all of these sentences using the new network and \n",
        "**Write** below how the new results compare to the previous ones.\n",
        "\n",
        "Just like before, if you encounter overfitting,\n",
        "train your model for more iterations, or change the hyperparameters in your\n",
        "model. You may need to do this even if your training accuracy is >=38%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1W2Vl3g6X54"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQeQXPfGQNB"
      },
      "source": [
        "**Write your explanation here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g36bTOV46X54"
      },
      "source": [
        "### Part (c) -- 4%\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8W6XrZ6X54"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fyrlDz6X55"
      },
      "source": [
        "## Question 4. Visualizing Word Embeddings (14%)\n",
        "\n",
        "While training the `PyTorchMLP`, we trained the `word_emb_layer`, which takes a one-hot\n",
        "representation of a word in our vocabulary, and returns a low-dimensional vector\n",
        "representation of that word. In this question, we will explore these word embeddings, which are a key concept in natural language processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Part (a) -- 4%\n",
        "\n",
        "The code below extracts the **weights** of the word embedding layer,\n",
        "and converts the PyTorch tensor into an numpy array.\n",
        "Explain why each *row* of `word_emb` contains the vector representing\n",
        "of a word. For example `word_emb[vocab_stoi[\"any\"],:]` contains the\n",
        "vector representation of the word \"any\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IvS6JDM6X55"
      },
      "source": [
        "word_emb_weights = list(pytorch_wordemb.word_emb_layer.parameters())[0]\n",
        "word_emb = word_emb_weights.detach().numpy().T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_kTjxrkonT"
      },
      "source": [
        "**Write your explanation here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-JenYz6X55"
      },
      "source": [
        "### Part (b) -- 5%\n",
        "\n",
        "One interesting thing about these word embeddings is that distances\n",
        "in these vector representations of words make some sense! To show this,\n",
        "we have provided code below that computes the *cosine similarity* of\n",
        "every pair of words in our vocabulary. This measure of similarity between vector ${\\bf v}$ and ${\\bf w}$ is defined as \n",
        "   $$d_{\\rm cos}({\\bf v},{\\bf w}) = \\frac{{\\bf v}^T{\\bf w}}{||{\\bf v}|| ||{\\bf w}||}.$$  We also pre-scale the vectors to have a unit norm, using Numpy's `norm` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPiv3pFX6X55"
      },
      "source": [
        "norms = np.linalg.norm(word_emb, axis=1)\n",
        "word_emb_norm = (word_emb.T / norms).T\n",
        "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "\n",
        "# Some example distances. The first one should be larger than the second\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['many']])\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['government']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ctM-Mgr6X55"
      },
      "source": [
        "Compute the 5 closest words to the following words:\n",
        "\n",
        "- \"four\"\n",
        "- \"go\"\n",
        "- \"what\"\n",
        "- \"should\"\n",
        "- \"school\"\n",
        "- \"your\"\n",
        "- \"yesterday\"\n",
        "- \"not\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66NCoAE26X55"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJAOG_pg6X55"
      },
      "source": [
        "### Part (c) -- 5%\n",
        "\n",
        "We can visualize the word embeddings by reducing the dimensionality of\n",
        "the word vectors to 2D. There are many dimensionality reduction techniques\n",
        "that we could use, and we will use an algorithm called t-SNE.\n",
        "(You don’t need to know what this is for the assignment; we will cover it later in the course.)\n",
        "Nearby points in this 2-D space are meant to correspond to nearby points\n",
        "in the original, high-dimensional space.\n",
        "\n",
        "The following code runs the t-SNE algorithm and plots the result.\n",
        "\n",
        "Look at the plot and find at least two clusters of related words.\n",
        "\n",
        "**Write** below for each cluster what is the commonality (if there is any) and if they make sense.\n",
        "\n",
        "Note that there is randomness in the initialization of the t-SNE \n",
        "algorithm. If you re-run this code, you may get a different image.\n",
        "Please make sure to submit your image in the PDF file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seD1PgwK6X56"
      },
      "source": [
        "import sklearn.manifold\n",
        "tsne = sklearn.manifold.TSNE()\n",
        "Y = tsne.fit_transform(word_emb)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.xlim(Y[:,0].min(), Y[:, 0].max())\n",
        "plt.ylim(Y[:,1].min(), Y[:, 1].max())\n",
        "for i, w in enumerate(vocab):\n",
        "    plt.text(Y[i, 0], Y[i, 1], w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb4gbqMam8S5"
      },
      "source": [
        "**Explain and discuss your results here:**"
      ]
    }
  ]
}